{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9391920,"sourceType":"datasetVersion","datasetId":5699506},{"sourceId":9401835,"sourceType":"datasetVersion","datasetId":5707343}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-15T15:31:38.759068Z","iopub.execute_input":"2024-09-15T15:31:38.759368Z","iopub.status.idle":"2024-09-15T15:31:39.180518Z","shell.execute_reply.started":"2024-09-15T15:31:38.759335Z","shell.execute_reply":"2024-09-15T15:31:39.179648Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/amazon-dataset/dataset/._sample_test_out.csv\n/kaggle/input/amazon-dataset/dataset/._train.csv\n/kaggle/input/amazon-dataset/dataset/._sample_test.csv\n/kaggle/input/amazon-dataset/dataset/._sample_test_out_fail.csv\n/kaggle/input/amazon-dataset/dataset/._test.csv\n/kaggle/input/amazon-dataset/src/._constants.py\n/kaggle/input/amazon-dataset/src/._utils.py\n/kaggle/input/amazon-dataset/src/._sanity.py\n/kaggle/input/amazon-dataset/src/._test.ipynb\n/kaggle/input/amazon-dataset/student_resource 3/._src\n/kaggle/input/amazon-dataset/student_resource 3/._sample_code.py\n/kaggle/input/amazon-dataset/student_resource 3/README.md\n/kaggle/input/amazon-dataset/student_resource 3/._README.md\n/kaggle/input/amazon-dataset/student_resource 3/._dataset\n/kaggle/input/amazon-dataset/student_resource 3/sample_code.py\n/kaggle/input/amazon-dataset/student_resource 3/dataset/sample_test.csv\n/kaggle/input/amazon-dataset/student_resource 3/dataset/._sample_test_out.csv\n/kaggle/input/amazon-dataset/student_resource 3/dataset/sample_test_out_fail.csv\n/kaggle/input/amazon-dataset/student_resource 3/dataset/sample_test_out.csv\n/kaggle/input/amazon-dataset/student_resource 3/dataset/._train.csv\n/kaggle/input/amazon-dataset/student_resource 3/dataset/._sample_test.csv\n/kaggle/input/amazon-dataset/student_resource 3/dataset/._sample_test_out_fail.csv\n/kaggle/input/amazon-dataset/student_resource 3/dataset/train.csv\n/kaggle/input/amazon-dataset/student_resource 3/dataset/test.csv\n/kaggle/input/amazon-dataset/student_resource 3/dataset/._test.csv\n/kaggle/input/amazon-dataset/student_resource 3/src/._constants.py\n/kaggle/input/amazon-dataset/student_resource 3/src/sanity.py\n/kaggle/input/amazon-dataset/student_resource 3/src/constants.py\n/kaggle/input/amazon-dataset/student_resource 3/src/._utils.py\n/kaggle/input/amazon-dataset/student_resource 3/src/test.ipynb\n/kaggle/input/amazon-dataset/student_resource 3/src/utils.py\n/kaggle/input/amazon-dataset/student_resource 3/src/._sanity.py\n/kaggle/input/amazon-dataset/student_resource 3/src/._test.ipynb\n/kaggle/input/bakloli/downloaded_image.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/amazon-dataset/student_resource 3/dataset/train.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T15:31:50.957910Z","iopub.execute_input":"2024-09-15T15:31:50.958287Z","iopub.status.idle":"2024-09-15T15:31:51.390226Z","shell.execute_reply.started":"2024-09-15T15:31:50.958248Z","shell.execute_reply":"2024-09-15T15:31:51.389348Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-15T15:31:51.391775Z","iopub.execute_input":"2024-09-15T15:31:51.392103Z","iopub.status.idle":"2024-09-15T15:31:51.410444Z","shell.execute_reply.started":"2024-09-15T15:31:51.392071Z","shell.execute_reply":"2024-09-15T15:31:51.409410Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                          image_link  group_id  entity_name  \\\n0  https://m.media-amazon.com/images/I/61I9XdN6OF...    748919  item_weight   \n1  https://m.media-amazon.com/images/I/71gSRbyXmo...    916768  item_volume   \n2  https://m.media-amazon.com/images/I/61BZ4zrjZX...    459516  item_weight   \n3  https://m.media-amazon.com/images/I/612mrlqiI4...    459516  item_weight   \n4  https://m.media-amazon.com/images/I/617Tl40LOX...    731432  item_weight   \n\n     entity_value  \n0      500.0 gram  \n1         1.0 cup  \n2      0.709 gram  \n3      0.709 gram  \n4  1400 milligram  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_link</th>\n      <th>group_id</th>\n      <th>entity_name</th>\n      <th>entity_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://m.media-amazon.com/images/I/61I9XdN6OF...</td>\n      <td>748919</td>\n      <td>item_weight</td>\n      <td>500.0 gram</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://m.media-amazon.com/images/I/71gSRbyXmo...</td>\n      <td>916768</td>\n      <td>item_volume</td>\n      <td>1.0 cup</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://m.media-amazon.com/images/I/61BZ4zrjZX...</td>\n      <td>459516</td>\n      <td>item_weight</td>\n      <td>0.709 gram</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://m.media-amazon.com/images/I/612mrlqiI4...</td>\n      <td>459516</td>\n      <td>item_weight</td>\n      <td>0.709 gram</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://m.media-amazon.com/images/I/617Tl40LOX...</td>\n      <td>731432</td>\n      <td>item_weight</td>\n      <td>1400 milligram</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"pip install easyocr","metadata":{"execution":{"iopub.status.busy":"2024-09-15T15:31:52.102917Z","iopub.execute_input":"2024-09-15T15:31:52.103305Z","iopub.status.idle":"2024-09-15T15:32:05.978940Z","shell.execute_reply.started":"2024-09-15T15:31:52.103267Z","shell.execute_reply":"2024-09-15T15:32:05.977861Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: easyocr in /opt/conda/lib/python3.10/site-packages (1.7.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from easyocr) (2.4.0)\nRequirement already satisfied: torchvision>=0.5 in /opt/conda/lib/python3.10/site-packages (from easyocr) (0.19.0)\nRequirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.10/site-packages (from easyocr) (4.10.0.84)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from easyocr) (1.14.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from easyocr) (1.26.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from easyocr) (9.5.0)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from easyocr) (0.23.2)\nRequirement already satisfied: python-bidi in /opt/conda/lib/python3.10/site-packages (from easyocr) (0.6.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from easyocr) (6.0.2)\nRequirement already satisfied: Shapely in /opt/conda/lib/python3.10/site-packages (from easyocr) (1.8.5.post1)\nRequirement already satisfied: pyclipper in /opt/conda/lib/python3.10/site-packages (from easyocr) (1.3.0.post5)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from easyocr) (1.11.1.1)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->easyocr) (3.3)\nRequirement already satisfied: imageio>=2.33 in /opt/conda/lib/python3.10/site-packages (from scikit-image->easyocr) (2.34.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->easyocr) (2024.5.22)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image->easyocr) (21.3)\nRequirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.10/site-packages (from scikit-image->easyocr) (0.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->easyocr) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->easyocr) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->easyocr) (1.13.2)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->easyocr) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->easyocr) (2024.6.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image->easyocr) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->easyocr) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->easyocr) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# For single image extraction","metadata":{}},{"cell_type":"markdown","source":"1) To see detailed extraction","metadata":{}},{"cell_type":"code","source":"\"\"\"import easyocr\nimport pandas as pd\nfrom PIL import Image\nimport numpy as np\nimport requests\nfrom io import BytesIO\nimport re\n\n# Initialize the EasyOCR reader (CPU mode if GPU is not available or has low memory)\nreader = easyocr.Reader(['en'], gpu=True)\n\n# Define regex patterns based on the allowed units\npatterns = {\n    'gram': r'(\\d+(\\.\\d+)?)\\s*(g|grams?)',\n    'kilogram': r'(\\d+(\\.\\d+)?)\\s*(kg|kilograms?)',\n    'milligram': r'(\\d+(\\.\\d+)?)\\s*(mg|milligrams?)',\n    'microgram': r'(\\d+(\\.\\d+)?)\\s*(µg|micrograms?)',\n    'ounce': r'(\\d+(\\.\\d+)?)\\s*(oz|ounces?)',\n    'pound': r'(\\d+(\\.\\d+)?)\\s*(lb|pounds?)',\n    'ton': r'(\\d+(\\.\\d+)?)\\s*(ton|tons?)',\n    'centimetre': r'(\\d+(\\.\\d+)?)\\s*(cm|centimetre|centimeters?)',\n    'foot': r'(\\d+(\\.\\d+)?)\\s*(ft|foot|feet)',\n    'inch': r'(\\d+(\\.\\d+)?)\\s*(in|inch|inches)',\n    'metre': r'(\\d+(\\.\\d+)?)\\s*(m|metre|meters?)',\n    'millimetre': r'(\\d+(\\.\\d+)?)\\s*(mm|millimetre|millimeters?)',\n    'yard': r'(\\d+(\\.\\d+)?)\\s*(yd|yards?)',\n    'kilovolt': r'(\\d+(\\.\\d+)?)\\s*(kv|kilovolts?)',\n    'millivolt': r'(\\d+(\\.\\d+)?)\\s*(mv|millivolts?)',\n    'volt': r'(\\d+(\\.\\d+)?)\\s*(v|volts?)',\n    'kilowatt': r'(\\d+(\\.\\d+)?)\\s*(kw|kilowatts?)',\n    'watt': r'(\\d+(\\.\\d+)?)\\s*(w|watts?)',\n    'centilitre': r'(\\d+(\\.\\d+)?)\\s*(cl|centilitres?)',\n    'cubic foot': r'(\\d+(\\.\\d+)?)\\s*(cubic foot|cubic feet)',\n    'cubic inch': r'(\\d+(\\.\\d+)?)\\s*(cubic inch|cubic inches)',\n    'cup': r'(\\d+(\\.\\d+)?)\\s*(cup|cups?)',\n    'decilitre': r'(\\d+(\\.\\d+)?)\\s*(dl|decilitres?)',\n    'fluid ounce': r'(\\d+(\\.\\d+)?)\\s*(fl oz|fluid ounces?)',\n    'gallon': r'(\\d+(\\.\\d+)?)\\s*(gallon|gallons?)',\n    'imperial gallon': r'(\\d+(\\.\\d+)?)\\s*(imperial gallon|imperial gallons?)',\n    'litre': r'(\\d+(\\.\\d+)?)\\s*(l|litre|liters?)',\n    'microlitre': r'(\\d+(\\.\\d+)?)\\s*(µl|microlitre|microlitres?)',\n    'millilitre': r'(\\d+(\\.\\d+)?)\\s*(ml|millilitre|millilitres?)',\n    'pint': r'(\\d+(\\.\\d+)?)\\s*(pt|pints?)',\n    'quart': r'(\\d+(\\.\\d+)?)\\s*(qt|quarts?)',\n}\n\n# Map entity names to their corresponding unit patterns\nentity_unit_map = {\n    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n    'item_weight': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n    'maximum_weight_recommendation': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n    'voltage': {'kilovolt', 'millivolt', 'volt'},\n    'wattage': {'kilowatt', 'watt'},\n    'item_volume': {'centilitre', 'cubic foot', 'cubic inch', 'cup', 'decilitre', 'fluid ounce', 'gallon', 'imperial gallon', 'litre', 'microlitre', 'millilitre', 'pint', 'quart'}\n}\n\n# Allowed units set based on entity_unit_map\nallowed_units = {unit for entity in entity_unit_map for unit in entity_unit_map[entity]}\n\n# Function to download an image from a URL\ndef download_image(url):\n    response = requests.get(url)\n    img = Image.open(BytesIO(response.content))\n    return img\n\n# Preprocessing function to clean up OCR text\ndef preprocess_text(text):\n    # Lowercase the text\n    text = text.lower()\n    \n    # Replace \"O\" with \"0\" (to handle common OCR errors like \"140OmG\")\n    text = text.replace(\"o\", \"0\")\n    \n    # Replace multiple spaces with a single space\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Remove any unnecessary punctuation that might interfere with matching\n    text = re.sub(r'[^\\w\\s.,]', '', text)\n    \n    return text\n\n# Function to extract various units from the text\ndef extract_units(text, entity_name):\n    extracted_units = set()\n\n    # Preprocess the text before extracting units\n    cleaned_text = preprocess_text(text)\n\n    if entity_name in entity_unit_map:\n        unit_list = entity_unit_map[entity_name]\n        for unit in unit_list:\n            if unit in patterns:\n                pattern = patterns[unit]\n                matches = re.findall(pattern, cleaned_text)\n                if matches:\n                    # Format as \"value unit\" and store unique entries\n                    for match in matches:\n                        value = float(match[0])\n                        formatted_entry = f\"{value} {unit}\"\n                        extracted_units.add(formatted_entry)\n\n    return extracted_units\n\n# Function to rotate the image and check OCR text\ndef rotate_and_extract(image):\n    # Initialize variables to track the best result\n    best_text = \"\"\n    best_angle = 0\n    \n    # Try OCR on the original and rotated images\n    for angle in [0, 90, 180, 270]:\n        rotated_img = image.rotate(angle, expand=True)\n        \n        # Convert the rotated image to a numpy array for easyocr\n        rotated_img_np = np.array(rotated_img)\n        \n        # Extract text from rotated image\n        img_text = reader.readtext(rotated_img_np)\n        final_text = \" \".join([text for _, text, __ in img_text])\n        \n        if len(final_text.strip()) > len(best_text.strip()):  # Choose the best text based on length\n            best_text = final_text\n            best_angle = angle\n    \n    # Return the best text and angle\n    return best_text, best_angle\n\n# Function to process each image URL\ndef process_image_url(image_url, entity_name):\n    # Download and process the image\n    image = download_image(image_url)\n    final_text, best_angle = rotate_and_extract(image)\n    \n    # Extract units from the final text based on the entity_name\n    units = extract_units(final_text, entity_name)\n    \n    return final_text, best_angle, units\n\n# Load the CSV file\ncsv_file_path = '/kaggle/input/amazon-dataset/student_resource 3/dataset/train.csv'\ndf = pd.read_csv(csv_file_path)\n\n# Limit to first 30 links for processing\ndf = df.head(30)\n\n# Initialize a list to store results\nresults = []\n\n# Iterate through each image URL in the CSV file\nfor index, row in df.iterrows():\n    image_url = row['image_link']  # Assuming the CSV column is named 'image_link'\n    entity_name = row['entity_name']  # Assuming this column specifies what entity to extract (e.g., weight, volume)\n    \n    print(f\"Processing image from URL: {image_url} for entity: {entity_name}\")\n    \n    final_text, best_angle, units = process_image_url(image_url, entity_name)\n    \n    # Prepare the result row\n    result_row = {\n        'image_link': image_url,\n        'entity_name': entity_name,\n        'extracted_text': final_text,\n        'best_angle': best_angle,\n        'predicted': list(units)  # Convert set to list for the predicted result\n    }\n    \n    # Append the result to the list\n    results.append(result_row)\n    \n    # Print results for each image\n    print(f\"Extracted Text (Best angle {best_angle} degrees): {final_text}\")\n    print(f\"Extracted Units for {entity_name}: {result_row['predicted']}\")\n    print(\"--------------------------------------------------\")\n\n# Create a new DataFrame from results and save it as a CSV\nresults_df = pd.DataFrame(results)\nresults_df.to_csv('predicted_units.csv', index=False)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-09-15T15:31:14.922090Z","iopub.execute_input":"2024-09-15T15:31:14.922440Z","iopub.status.idle":"2024-09-15T15:31:14.938367Z","shell.execute_reply.started":"2024-09-15T15:31:14.922403Z","shell.execute_reply":"2024-09-15T15:31:14.937289Z"},"trusted":true},"execution_count":223,"outputs":[{"execution_count":223,"output_type":"execute_result","data":{"text/plain":"'import easyocr\\nimport pandas as pd\\nfrom PIL import Image\\nimport numpy as np\\nimport requests\\nfrom io import BytesIO\\nimport re\\n\\n# Initialize the EasyOCR reader (CPU mode if GPU is not available or has low memory)\\nreader = easyocr.Reader([\\'en\\'], gpu=True)\\n\\n# Define regex patterns based on the allowed units\\npatterns = {\\n    \\'gram\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(g|grams?)\\',\\n    \\'kilogram\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(kg|kilograms?)\\',\\n    \\'milligram\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(mg|milligrams?)\\',\\n    \\'microgram\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(µg|micrograms?)\\',\\n    \\'ounce\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(oz|ounces?)\\',\\n    \\'pound\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(lb|pounds?)\\',\\n    \\'ton\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(ton|tons?)\\',\\n    \\'centimetre\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(cm|centimetre|centimeters?)\\',\\n    \\'foot\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(ft|foot|feet)\\',\\n    \\'inch\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(in|inch|inches)\\',\\n    \\'metre\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(m|metre|meters?)\\',\\n    \\'millimetre\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(mm|millimetre|millimeters?)\\',\\n    \\'yard\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(yd|yards?)\\',\\n    \\'kilovolt\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(kv|kilovolts?)\\',\\n    \\'millivolt\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(mv|millivolts?)\\',\\n    \\'volt\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(v|volts?)\\',\\n    \\'kilowatt\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(kw|kilowatts?)\\',\\n    \\'watt\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(w|watts?)\\',\\n    \\'centilitre\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(cl|centilitres?)\\',\\n    \\'cubic foot\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(cubic foot|cubic feet)\\',\\n    \\'cubic inch\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(cubic inch|cubic inches)\\',\\n    \\'cup\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(cup|cups?)\\',\\n    \\'decilitre\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(dl|decilitres?)\\',\\n    \\'fluid ounce\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(fl oz|fluid ounces?)\\',\\n    \\'gallon\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(gallon|gallons?)\\',\\n    \\'imperial gallon\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(imperial gallon|imperial gallons?)\\',\\n    \\'litre\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(l|litre|liters?)\\',\\n    \\'microlitre\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(µl|microlitre|microlitres?)\\',\\n    \\'millilitre\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(ml|millilitre|millilitres?)\\',\\n    \\'pint\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(pt|pints?)\\',\\n    \\'quart\\': r\\'(\\\\d+(\\\\.\\\\d+)?)\\\\s*(qt|quarts?)\\',\\n}\\n\\n# Map entity names to their corresponding unit patterns\\nentity_unit_map = {\\n    \\'width\\': {\\'centimetre\\', \\'foot\\', \\'inch\\', \\'metre\\', \\'millimetre\\', \\'yard\\'},\\n    \\'depth\\': {\\'centimetre\\', \\'foot\\', \\'inch\\', \\'metre\\', \\'millimetre\\', \\'yard\\'},\\n    \\'height\\': {\\'centimetre\\', \\'foot\\', \\'inch\\', \\'metre\\', \\'millimetre\\', \\'yard\\'},\\n    \\'item_weight\\': {\\'gram\\', \\'kilogram\\', \\'microgram\\', \\'milligram\\', \\'ounce\\', \\'pound\\', \\'ton\\'},\\n    \\'maximum_weight_recommendation\\': {\\'gram\\', \\'kilogram\\', \\'microgram\\', \\'milligram\\', \\'ounce\\', \\'pound\\', \\'ton\\'},\\n    \\'voltage\\': {\\'kilovolt\\', \\'millivolt\\', \\'volt\\'},\\n    \\'wattage\\': {\\'kilowatt\\', \\'watt\\'},\\n    \\'item_volume\\': {\\'centilitre\\', \\'cubic foot\\', \\'cubic inch\\', \\'cup\\', \\'decilitre\\', \\'fluid ounce\\', \\'gallon\\', \\'imperial gallon\\', \\'litre\\', \\'microlitre\\', \\'millilitre\\', \\'pint\\', \\'quart\\'}\\n}\\n\\n# Allowed units set based on entity_unit_map\\nallowed_units = {unit for entity in entity_unit_map for unit in entity_unit_map[entity]}\\n\\n# Function to download an image from a URL\\ndef download_image(url):\\n    response = requests.get(url)\\n    img = Image.open(BytesIO(response.content))\\n    return img\\n\\n# Preprocessing function to clean up OCR text\\ndef preprocess_text(text):\\n    # Lowercase the text\\n    text = text.lower()\\n    \\n    # Replace \"O\" with \"0\" (to handle common OCR errors like \"140OmG\")\\n    text = text.replace(\"o\", \"0\")\\n    \\n    # Replace multiple spaces with a single space\\n    text = re.sub(r\\'\\\\s+\\', \\' \\', text)\\n    \\n    # Remove any unnecessary punctuation that might interfere with matching\\n    text = re.sub(r\\'[^\\\\w\\\\s.,]\\', \\'\\', text)\\n    \\n    return text\\n\\n# Function to extract various units from the text\\ndef extract_units(text, entity_name):\\n    extracted_units = set()\\n\\n    # Preprocess the text before extracting units\\n    cleaned_text = preprocess_text(text)\\n\\n    if entity_name in entity_unit_map:\\n        unit_list = entity_unit_map[entity_name]\\n        for unit in unit_list:\\n            if unit in patterns:\\n                pattern = patterns[unit]\\n                matches = re.findall(pattern, cleaned_text)\\n                if matches:\\n                    # Format as \"value unit\" and store unique entries\\n                    for match in matches:\\n                        value = float(match[0])\\n                        formatted_entry = f\"{value} {unit}\"\\n                        extracted_units.add(formatted_entry)\\n\\n    return extracted_units\\n\\n# Function to rotate the image and check OCR text\\ndef rotate_and_extract(image):\\n    # Initialize variables to track the best result\\n    best_text = \"\"\\n    best_angle = 0\\n    \\n    # Try OCR on the original and rotated images\\n    for angle in [0, 90, 180, 270]:\\n        rotated_img = image.rotate(angle, expand=True)\\n        \\n        # Convert the rotated image to a numpy array for easyocr\\n        rotated_img_np = np.array(rotated_img)\\n        \\n        # Extract text from rotated image\\n        img_text = reader.readtext(rotated_img_np)\\n        final_text = \" \".join([text for _, text, __ in img_text])\\n        \\n        if len(final_text.strip()) > len(best_text.strip()):  # Choose the best text based on length\\n            best_text = final_text\\n            best_angle = angle\\n    \\n    # Return the best text and angle\\n    return best_text, best_angle\\n\\n# Function to process each image URL\\ndef process_image_url(image_url, entity_name):\\n    # Download and process the image\\n    image = download_image(image_url)\\n    final_text, best_angle = rotate_and_extract(image)\\n    \\n    # Extract units from the final text based on the entity_name\\n    units = extract_units(final_text, entity_name)\\n    \\n    return final_text, best_angle, units\\n\\n# Load the CSV file\\ncsv_file_path = \\'/kaggle/input/amazon-dataset/student_resource 3/dataset/train.csv\\'\\ndf = pd.read_csv(csv_file_path)\\n\\n# Limit to first 30 links for processing\\ndf = df.head(30)\\n\\n# Initialize a list to store results\\nresults = []\\n\\n# Iterate through each image URL in the CSV file\\nfor index, row in df.iterrows():\\n    image_url = row[\\'image_link\\']  # Assuming the CSV column is named \\'image_link\\'\\n    entity_name = row[\\'entity_name\\']  # Assuming this column specifies what entity to extract (e.g., weight, volume)\\n    \\n    print(f\"Processing image from URL: {image_url} for entity: {entity_name}\")\\n    \\n    final_text, best_angle, units = process_image_url(image_url, entity_name)\\n    \\n    # Prepare the result row\\n    result_row = {\\n        \\'image_link\\': image_url,\\n        \\'entity_name\\': entity_name,\\n        \\'extracted_text\\': final_text,\\n        \\'best_angle\\': best_angle,\\n        \\'predicted\\': list(units)  # Convert set to list for the predicted result\\n    }\\n    \\n    # Append the result to the list\\n    results.append(result_row)\\n    \\n    # Print results for each image\\n    print(f\"Extracted Text (Best angle {best_angle} degrees): {final_text}\")\\n    print(f\"Extracted Units for {entity_name}: {result_row[\\'predicted\\']}\")\\n    print(\"--------------------------------------------------\")\\n\\n# Create a new DataFrame from results and save it as a CSV\\nresults_df = pd.DataFrame(results)\\nresults_df.to_csv(\\'predicted_units.csv\\', index=False)\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"2)Didn't want to see details","metadata":{}},{"cell_type":"code","source":"import easyocr\nimport pandas as pd\nfrom PIL import Image\nimport numpy as np\nimport requests\nfrom io import BytesIO\nimport re\nimport warnings\n\n# Suppress FutureWarnings and other warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# Initialize the EasyOCR reader (CPU mode if GPU is not available or has low memory)\nreader = easyocr.Reader(['en'], gpu=True)\n\n# Define regex patterns based on the allowed units\npatterns = {\n    'gram': r'(\\d+(\\.\\d+)?)\\s*(g|grams?)',\n    'kilogram': r'(\\d+(\\.\\d+)?)\\s*(kg|kilograms?)',\n    'milligram': r'(\\d+(\\.\\d+)?)\\s*(mg|milligrams?)',\n    'microgram': r'(\\d+(\\.\\d+)?)\\s*(µg|micrograms?)',\n    'ounce': r'(\\d+(\\.\\d+)?)\\s*(oz|ounces?)',\n    'pound': r'(\\d+(\\.\\d+)?)\\s*(lb|pounds?)',\n    'ton': r'(\\d+(\\.\\d+)?)\\s*(ton|tons?)',\n    'centimetre': r'(\\d+(\\.\\d+)?)\\s*(cm|centimetre|centimeters?)',\n    'foot': r'(\\d+(\\.\\d+)?)\\s*(ft|foot|feet)',\n    'inch': r'(\\d+(\\.\\d+)?)\\s*(in|inch|inches)',\n    'metre': r'(\\d+(\\.\\d+)?)\\s*(m|metre|meters?)',\n    'millimetre': r'(\\d+(\\.\\d+)?)\\s*(mm|millimetre|millimeters?)',\n    'yard': r'(\\d+(\\.\\d+)?)\\s*(yd|yards?)',\n    'kilovolt': r'(\\d+(\\.\\d+)?)\\s*(kv|kilovolts?)',\n    'millivolt': r'(\\d+(\\.\\d+)?)\\s*(mv|millivolts?)',\n    'volt': r'(\\d+(\\.\\d+)?)\\s*(v|volts?)',\n    'kilowatt': r'(\\d+(\\.\\d+)?)\\s*(kw|kilowatts?)',\n    'watt': r'(\\d+(\\.\\d+)?)\\s*(w|watts?)',\n    'centilitre': r'(\\d+(\\.\\d+)?)\\s*(cl|centilitres?)',\n    'cubic foot': r'(\\d+(\\.\\d+)?)\\s*(cubic foot|cubic feet)',\n    'cubic inch': r'(\\d+(\\.\\d+)?)\\s*(cubic inch|cubic inches)',\n    'cup': r'(\\d+(\\.\\d+)?)\\s*(cup|cups?)',\n    'decilitre': r'(\\d+(\\.\\d+)?)\\s*(dl|decilitres?)',\n    'fluid ounce': r'(\\d+(\\.\\d+)?)\\s*(fl oz|fluid ounces?)',\n    'gallon': r'(\\d+(\\.\\d+)?)\\s*(gallon|gallons?)',\n    'imperial gallon': r'(\\d+(\\.\\d+)?)\\s*(imperial gallon|imperial gallons?)',\n    'litre': r'(\\d+(\\.\\d+)?)\\s*(l|litre|liters?)',\n    'microlitre': r'(\\d+(\\.\\d+)?)\\s*(µl|microlitre|microlitres?)',\n    'millilitre': r'(\\d+(\\.\\d+)?)\\s*(ml|millilitre|millilitres?)',\n    'pint': r'(\\d+(\\.\\d+)?)\\s*(pt|pints?)',\n    'quart': r'(\\d+(\\.\\d+)?)\\s*(qt|quarts?)',\n}\n\n# Map entity names to their corresponding unit patterns\nentity_unit_map = {\n    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n    'item_weight': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n    'maximum_weight_recommendation': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n    'voltage': {'kilovolt', 'millivolt', 'volt'},\n    'wattage': {'kilowatt', 'watt'},\n    'item_volume': {'centilitre', 'cubic foot', 'cubic inch', 'cup', 'decilitre', 'fluid ounce', 'gallon', 'imperial gallon', 'litre', 'microlitre', 'millilitre', 'pint', 'quart'}\n}\n\n# Allowed units set based on entity_unit_map\nallowed_units = {unit for entity in entity_unit_map for unit in entity_unit_map[entity]}\n\n# Function to download an image from a URL\ndef download_image(url):\n    response = requests.get(url)\n    img = Image.open(BytesIO(response.content))\n    return img\n\n# Preprocessing function to clean up OCR text\ndef preprocess_text(text):\n    # Lowercase the text\n    text = text.lower()\n    \n    # Replace \"O\" with \"0\" (to handle common OCR errors like \"140OmG\")\n    text = text.replace(\"o\", \"0\")\n    \n    # Replace multiple spaces with a single space\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Remove any unnecessary punctuation that might interfere with matching\n    text = re.sub(r'[^\\w\\s.,]', '', text)\n    \n    return text\n\n# Function to extract various units from the text\ndef extract_units(text, entity_name):\n    extracted_units = set()\n\n    # Preprocess the text before extracting units\n    cleaned_text = preprocess_text(text)\n\n    if entity_name in entity_unit_map:\n        unit_list = entity_unit_map[entity_name]\n        for unit in unit_list:\n            if unit in patterns:\n                pattern = patterns[unit]\n                matches = re.findall(pattern, cleaned_text)\n                if matches:\n                    # Format as \"value unit\" and store unique entries\n                    for match in matches:\n                        value = float(match[0])\n                        formatted_entry = f\"{value} {unit}\"\n                        extracted_units.add(formatted_entry)\n\n    return extracted_units\n\n# Function to rotate the image and check OCR text\ndef rotate_and_extract(image):\n    # Initialize variables to track the best result\n    best_text = \"\"\n    best_angle = 0\n    \n    # Try OCR on the original and rotated images\n    for angle in [0, 90, 180, 270]:\n        rotated_img = image.rotate(angle, expand=True)\n        \n        # Convert the rotated image to a numpy array for easyocr\n        rotated_img_np = np.array(rotated_img)\n        \n        # Extract text from rotated image\n        img_text = reader.readtext(rotated_img_np)\n        final_text = \" \".join([text for _, text, __ in img_text])\n        \n        if len(final_text.strip()) > len(best_text.strip()):  # Choose the best text based on length\n            best_text = final_text\n            best_angle = angle\n    \n    # Return the best text and angle\n    return best_text, best_angle\n\n# Function to process each image URL\ndef process_image_url(image_url, entity_name):\n    # Download and process the image\n    image = download_image(image_url)\n    final_text, best_angle = rotate_and_extract(image)\n    \n    # Extract units from the final text based on the entity_name\n    units = extract_units(final_text, entity_name)\n    \n    return final_text, best_angle, units\n\n# Load the CSV file\ncsv_file_path = '/kaggle/input/amazon-dataset/student_resource 3/dataset/train.csv'\ndf = pd.read_csv(csv_file_path)\n\n# Limit to first 50 links for processing\ndf = df.head(50)\n\n# Initialize a list to store results\nresults = []\n\n# Iterate through each image URL in the CSV file\nfor index, row in df.iterrows():\n    image_url = row['image_link']  # Assuming the CSV column is named 'image_link'\n    entity_name = row['entity_name']  # Assuming this column specifies what entity to extract (e.g., weight, volume)\n    \n    # Process image\n    final_text, best_angle, units = process_image_url(image_url, entity_name)\n    \n    # Prepare the result row\n    result_row = {\n        'image_link': image_url,\n        'entity_name': entity_name,\n        'extracted_text': final_text,\n        'best_angle': best_angle,\n        'predicted': list(units)  # Convert set to list for the predicted result\n    }\n    \n    # Append the result to the list\n    results.append(result_row)\n\n# Print the count of processed images\nprint(f\"Processed {len(results)} images.\")\n\n# Create a new DataFrame from results and save it as a CSV\nresults_df = pd.DataFrame(results)\nresults_df.to_csv('predicted_units.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T15:32:10.124915Z","iopub.execute_input":"2024-09-15T15:32:10.125291Z","iopub.status.idle":"2024-09-15T15:37:33.679079Z","shell.execute_reply.started":"2024-09-15T15:32:10.125255Z","shell.execute_reply":"2024-09-15T15:37:33.678160Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Processed 50 images.\n","output_type":"stream"}]},{"cell_type":"code","source":"pred = pd.read_csv('predicted_units.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T16:05:46.442783Z","iopub.execute_input":"2024-09-15T16:05:46.443406Z","iopub.status.idle":"2024-09-15T16:05:46.450443Z","shell.execute_reply.started":"2024-09-15T16:05:46.443364Z","shell.execute_reply":"2024-09-15T16:05:46.449396Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"new_train = pd.read_csv('/kaggle/input/amazon-dataset/student_resource 3/dataset/train.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T16:05:47.574525Z","iopub.execute_input":"2024-09-15T16:05:47.574902Z","iopub.status.idle":"2024-09-15T16:05:47.972336Z","shell.execute_reply.started":"2024-09-15T16:05:47.574868Z","shell.execute_reply":"2024-09-15T16:05:47.971396Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Merging df1 and test1 on image_link, entity_name, and group_id\npred = pred.merge(new_train, on=['image_link', 'entity_name'], how='inner')\n\n# Optionally, if you need to remove duplicates based on these columns:\npred = pred.drop_duplicates(subset=['image_link', 'entity_name', 'group_id'])","metadata":{"execution":{"iopub.status.busy":"2024-09-15T16:05:47.984000Z","iopub.execute_input":"2024-09-15T16:05:47.984316Z","iopub.status.idle":"2024-09-15T16:05:48.144925Z","shell.execute_reply.started":"2024-09-15T16:05:47.984285Z","shell.execute_reply":"2024-09-15T16:05:48.144139Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Convert 'predicted' column from a list of strings to a single string without brackets and quotes\npred['predicted'] = pred['predicted'].apply(lambda x: ', '.join(sorted(set(x.strip('[]').replace(\"'\", \"\").split(', ')))))","metadata":{"execution":{"iopub.status.busy":"2024-09-15T16:05:48.558706Z","iopub.execute_input":"2024-09-15T16:05:48.559093Z","iopub.status.idle":"2024-09-15T16:05:48.564969Z","shell.execute_reply.started":"2024-09-15T16:05:48.559054Z","shell.execute_reply":"2024-09-15T16:05:48.564035Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"pred = pred[['image_link', 'entity_name', 'extracted_text', 'predicted', 'group_id', 'entity_value']]","metadata":{"execution":{"iopub.status.busy":"2024-09-15T16:17:19.918654Z","iopub.execute_input":"2024-09-15T16:17:19.919025Z","iopub.status.idle":"2024-09-15T16:17:19.924952Z","shell.execute_reply.started":"2024-09-15T16:17:19.918989Z","shell.execute_reply":"2024-09-15T16:17:19.923916Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport re\n\n# Function to convert values to a common unit (e.g., grams) for comparison\ndef convert_to_grams(value):\n    if pd.isna(value):\n        return None\n    value = value.lower().strip()\n    # Extract the numeric part and unit\n    match = re.match(r\"([\\d.]+)\\s*(\\w+)\", value)\n    if match:\n        number = float(match.group(1))\n        unit = match.group(2)\n        if unit in [\"gram\", \"grams\", \"g\"]:\n            return number\n        elif unit in [\"milligram\", \"milligrams\", \"mg\"]:\n            return number / 1000\n        elif unit in [\"kilogram\", \"kilograms\", \"kg\"]:\n            return number * 1000\n        elif unit in [\"ounce\", \"ounces\", \"oz\"]:\n            return number * 28.3495\n        elif unit in [\"pound\", \"pounds\", \"lb\"]:\n            return number * 453.592\n    return None\n\n# Function to get the highest value in grams and return the original value\ndef get_highest_value(row):\n    predicted_values = row['predicted'].split(',')\n    highest_value = None\n    original_value = None\n    for value in predicted_values:\n        value_in_grams = convert_to_grams(value)\n        if value_in_grams is not None:\n            if highest_value is None or value_in_grams > highest_value:\n                highest_value = value_in_grams\n                original_value = value.strip()\n    return original_value if original_value is not None else \"\"\n\n# Apply the function to create a new column 'predict'\npred['predict'] = pred.apply(get_highest_value, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T16:16:47.581717Z","iopub.execute_input":"2024-09-15T16:16:47.582482Z","iopub.status.idle":"2024-09-15T16:16:47.595905Z","shell.execute_reply.started":"2024-09-15T16:16:47.582440Z","shell.execute_reply":"2024-09-15T16:16:47.594963Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/1197466700.py:40: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  pred['predict'] = pred.apply(get_highest_value, axis=1)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Display the cleaned DataFrame\npred.head(30)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T16:17:23.335458Z","iopub.execute_input":"2024-09-15T16:17:23.336309Z","iopub.status.idle":"2024-09-15T16:17:23.354561Z","shell.execute_reply.started":"2024-09-15T16:17:23.336270Z","shell.execute_reply":"2024-09-15T16:17:23.353475Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"                                           image_link  entity_name  \\\n0   https://m.media-amazon.com/images/I/61I9XdN6OF...  item_weight   \n1   https://m.media-amazon.com/images/I/71gSRbyXmo...  item_volume   \n2   https://m.media-amazon.com/images/I/61BZ4zrjZX...  item_weight   \n3   https://m.media-amazon.com/images/I/612mrlqiI4...  item_weight   \n4   https://m.media-amazon.com/images/I/617Tl40LOX...  item_weight   \n5   https://m.media-amazon.com/images/I/61QsBSE7jg...  item_weight   \n6   https://m.media-amazon.com/images/I/81xsq6vf2q...  item_weight   \n7   https://m.media-amazon.com/images/I/71DiLRHeZd...  item_weight   \n8   https://m.media-amazon.com/images/I/91Cma3Rzse...  item_weight   \n9   https://m.media-amazon.com/images/I/71jBLhmTNl...  item_weight   \n10  https://m.media-amazon.com/images/I/81N73b5khV...  item_weight   \n11  https://m.media-amazon.com/images/I/61oMj2iXOu...  item_weight   \n12  https://m.media-amazon.com/images/I/91LPf6OjV9...  item_weight   \n13  https://m.media-amazon.com/images/I/81fOxWWWKY...  item_weight   \n14  https://m.media-amazon.com/images/I/81dzao1Ob4...  item_weight   \n15  https://m.media-amazon.com/images/I/91-iahVGED...  item_weight   \n16  https://m.media-amazon.com/images/I/81S2+GnYpT...  item_weight   \n17  https://m.media-amazon.com/images/I/81e2YtCOKv...  item_weight   \n18  https://m.media-amazon.com/images/I/81RNsNEM1E...  item_weight   \n19  https://m.media-amazon.com/images/I/91prZeizZn...  item_weight   \n20  https://m.media-amazon.com/images/I/31EvJszFVf...  item_weight   \n21  https://m.media-amazon.com/images/I/61wzlucTRE...  item_volume   \n22  https://m.media-amazon.com/images/I/61sQ+qAKr4...  item_weight   \n23  https://m.media-amazon.com/images/I/81x77l2T5N...  item_weight   \n24  https://m.media-amazon.com/images/I/71nywfWZUw...  item_weight   \n25  https://m.media-amazon.com/images/I/71nywfWZUw...      voltage   \n26  https://m.media-amazon.com/images/I/51WsuKKAVr...  item_weight   \n27  https://m.media-amazon.com/images/I/61XGDKap+J...  item_weight   \n28  https://m.media-amazon.com/images/I/715vVcWJxG...  item_weight   \n29  https://m.media-amazon.com/images/I/613v+2W4Uw...  item_weight   \n\n                                       extracted_text  \\\n0   PROPOS' NATUREJ INGREDIENT MENAGER MULTI-USAGE...   \n1   TLael=_ 767222 Xe RRIFIC; LEBENSMITTELECHT G D...   \n2   COMPOSITION Serving Size: Tablet (0.709 g) Eac...   \n3   conpositioN Serving Size: Tablet (0.709 g1 Eac...   \n4   Horbaach' HIGH StRENGTH PSYLLIUM HUSK PLANTAGO...   \n5   4JEEQJOH sue1ue1aba, 3 suebaA JOJ aqens - buiu...   \n6   HOrbaach Directions: For adults, take two (2) ...   \n7   VEGAN WHEAT FREE soy FREE OVATA DAIRY FREE FRE...   \n8   Horbaach V 100% HIGHEST QUALITY TO HGH GMP ) H...   \n9   NEW LOOK SAME TRUSTEDQUALITY OLD Horbaach NEW ...   \n10  GroBe Kapazilat; Tragialigkeit bis zu 30KG Das...   \n11  6XSt < 6XSt-GZ 6XSZ-GL 6XGl-0l 6x01 > UD 91 WD...   \n12  K Helps in osteoarthritis Kelps in digestion R...   \n13  (zo €s\"€) 6 00L JHOIJM LN Sasvasia SLVaWo)+ NO...   \n14  PACKAGING CHANGED FOOD FOR THE MODERN LIFE SUP...   \n15  X Loose packaging X Impurities added 7} Other ...   \n16  # Glucon-D FREE' Regutarz0g Ipack Glucon-D 1 I...   \n17  37 22 (uJel J0 wogoq mojaq ajeds &38) (S3XL 7I...   \n18  # Glucon-D GREE' Reejucarzo0g Glucon-0 1 Insta...   \n19  # Glucon-D FREE' Regjucor20g Ipack Glucon-D 1 ...   \n20                         iEE Glucon- 0 9.4 in 23 cm   \n21  ONE SIZE FITS ALL STRETCHABLE AND BLACK FABRIC...   \n22  925 Sterling Silver 18K Gold Plated 0.28 INCH ...   \n23  Crealig ; ( sustainable Valpednkatare 1 koryou...   \n24  9.1cm/3.58\" 36.8cm/ 14.48\" 481 13Ah 481 17BBAh...   \n25  9.1cm/3.58\" 36.8cm/ 14.48\" 481 13Ah 481 17BBAh...   \n26  Product Name Harvest Festival Doll Product Siz...   \n27                                      Hll Fbgt 885T   \n28  Consistens NUVIDA NATURAL HEALTH CRANBERRY EXT...   \n29  3 Jbapu3M'8ni *0-43 uedoulsbunoepibn Uilorjp3 ...   \n\n                                            predicted  group_id  \\\n0                                          500.0 gram    748919   \n1                                                        916768   \n2   0.04 gram, 0.09 gram, 0.2 gram, 0.51 gram, 0.7...    459516   \n3   0.2 gram, 0.51 gram, 0.709 gram, 10.0 milligra...    459516   \n4                                    1400.0 milligram    731432   \n5                                                        731432   \n6                                    1400.0 milligram    731432   \n7                                    1400.0 milligram    731432   \n8                                    1400.0 milligram    731432   \n9                   1400.0 milligram, 400.0 milligram    731432   \n10                                      30.0 kilogram    149159   \n11                                                       308856   \n12                                         100.0 gram    281678   \n13                                                       281678   \n14                               0.0 gram, 100.0 gram    281678   \n15                                                       281678   \n16                                           0.0 gram    731432   \n17                                 0.0 gram, 2.0 gram    731432   \n18  0.0 gram, 10.0 gram, 170.0 milligram, 31.5 gra...    731432   \n19                                20.0 gram, 6.0 gram    731432   \n20                                                       731432   \n21                                          0.0 litre    252585   \n22                                           2.7 gram    299791   \n23                              112.0 gram, 28.0 gram    884560   \n24                                      44.0 kilogram    179080   \n25                                          48.0 volt    179080   \n26                                         158.0 gram    866516   \n27                                                       866516   \n28                        5000.0 milligram, 90.0 gram    459516   \n29                                                       524635   \n\n                  entity_value  \n0                   500.0 gram  \n1                      1.0 cup  \n2                   0.709 gram  \n3                   0.709 gram  \n4               1400 milligram  \n5               1400 milligram  \n6               1400 milligram  \n7               1400 milligram  \n8               1400 milligram  \n9               1400 milligram  \n10               30.0 kilogram  \n11  10 kilogram to 15 kilogram  \n12                  3.53 ounce  \n13                  3.53 ounce  \n14                    53 ounce  \n15                    100 gram  \n16                    200 gram  \n17                  1 kilogram  \n18                    200 gram  \n19                    200 gram  \n20                    200 gram  \n21                  4.0 gallon  \n22                    2.7 gram  \n23                    112 gram  \n24                4.1 kilogram  \n25                   48.0 volt  \n26                  158.0 gram  \n27                  158.0 gram  \n28              5000 milligram  \n29                  18.55 gram  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_link</th>\n      <th>entity_name</th>\n      <th>extracted_text</th>\n      <th>predicted</th>\n      <th>group_id</th>\n      <th>entity_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://m.media-amazon.com/images/I/61I9XdN6OF...</td>\n      <td>item_weight</td>\n      <td>PROPOS' NATUREJ INGREDIENT MENAGER MULTI-USAGE...</td>\n      <td>500.0 gram</td>\n      <td>748919</td>\n      <td>500.0 gram</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://m.media-amazon.com/images/I/71gSRbyXmo...</td>\n      <td>item_volume</td>\n      <td>TLael=_ 767222 Xe RRIFIC; LEBENSMITTELECHT G D...</td>\n      <td></td>\n      <td>916768</td>\n      <td>1.0 cup</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://m.media-amazon.com/images/I/61BZ4zrjZX...</td>\n      <td>item_weight</td>\n      <td>COMPOSITION Serving Size: Tablet (0.709 g) Eac...</td>\n      <td>0.04 gram, 0.09 gram, 0.2 gram, 0.51 gram, 0.7...</td>\n      <td>459516</td>\n      <td>0.709 gram</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://m.media-amazon.com/images/I/612mrlqiI4...</td>\n      <td>item_weight</td>\n      <td>conpositioN Serving Size: Tablet (0.709 g1 Eac...</td>\n      <td>0.2 gram, 0.51 gram, 0.709 gram, 10.0 milligra...</td>\n      <td>459516</td>\n      <td>0.709 gram</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://m.media-amazon.com/images/I/617Tl40LOX...</td>\n      <td>item_weight</td>\n      <td>Horbaach' HIGH StRENGTH PSYLLIUM HUSK PLANTAGO...</td>\n      <td>1400.0 milligram</td>\n      <td>731432</td>\n      <td>1400 milligram</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>https://m.media-amazon.com/images/I/61QsBSE7jg...</td>\n      <td>item_weight</td>\n      <td>4JEEQJOH sue1ue1aba, 3 suebaA JOJ aqens - buiu...</td>\n      <td></td>\n      <td>731432</td>\n      <td>1400 milligram</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>https://m.media-amazon.com/images/I/81xsq6vf2q...</td>\n      <td>item_weight</td>\n      <td>HOrbaach Directions: For adults, take two (2) ...</td>\n      <td>1400.0 milligram</td>\n      <td>731432</td>\n      <td>1400 milligram</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>https://m.media-amazon.com/images/I/71DiLRHeZd...</td>\n      <td>item_weight</td>\n      <td>VEGAN WHEAT FREE soy FREE OVATA DAIRY FREE FRE...</td>\n      <td>1400.0 milligram</td>\n      <td>731432</td>\n      <td>1400 milligram</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>https://m.media-amazon.com/images/I/91Cma3Rzse...</td>\n      <td>item_weight</td>\n      <td>Horbaach V 100% HIGHEST QUALITY TO HGH GMP ) H...</td>\n      <td>1400.0 milligram</td>\n      <td>731432</td>\n      <td>1400 milligram</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>https://m.media-amazon.com/images/I/71jBLhmTNl...</td>\n      <td>item_weight</td>\n      <td>NEW LOOK SAME TRUSTEDQUALITY OLD Horbaach NEW ...</td>\n      <td>1400.0 milligram, 400.0 milligram</td>\n      <td>731432</td>\n      <td>1400 milligram</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>https://m.media-amazon.com/images/I/81N73b5khV...</td>\n      <td>item_weight</td>\n      <td>GroBe Kapazilat; Tragialigkeit bis zu 30KG Das...</td>\n      <td>30.0 kilogram</td>\n      <td>149159</td>\n      <td>30.0 kilogram</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>https://m.media-amazon.com/images/I/61oMj2iXOu...</td>\n      <td>item_weight</td>\n      <td>6XSt &lt; 6XSt-GZ 6XSZ-GL 6XGl-0l 6x01 &gt; UD 91 WD...</td>\n      <td></td>\n      <td>308856</td>\n      <td>10 kilogram to 15 kilogram</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>https://m.media-amazon.com/images/I/91LPf6OjV9...</td>\n      <td>item_weight</td>\n      <td>K Helps in osteoarthritis Kelps in digestion R...</td>\n      <td>100.0 gram</td>\n      <td>281678</td>\n      <td>3.53 ounce</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>https://m.media-amazon.com/images/I/81fOxWWWKY...</td>\n      <td>item_weight</td>\n      <td>(zo €s\"€) 6 00L JHOIJM LN Sasvasia SLVaWo)+ NO...</td>\n      <td></td>\n      <td>281678</td>\n      <td>3.53 ounce</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>https://m.media-amazon.com/images/I/81dzao1Ob4...</td>\n      <td>item_weight</td>\n      <td>PACKAGING CHANGED FOOD FOR THE MODERN LIFE SUP...</td>\n      <td>0.0 gram, 100.0 gram</td>\n      <td>281678</td>\n      <td>53 ounce</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>https://m.media-amazon.com/images/I/91-iahVGED...</td>\n      <td>item_weight</td>\n      <td>X Loose packaging X Impurities added 7} Other ...</td>\n      <td></td>\n      <td>281678</td>\n      <td>100 gram</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>https://m.media-amazon.com/images/I/81S2+GnYpT...</td>\n      <td>item_weight</td>\n      <td># Glucon-D FREE' Regutarz0g Ipack Glucon-D 1 I...</td>\n      <td>0.0 gram</td>\n      <td>731432</td>\n      <td>200 gram</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>https://m.media-amazon.com/images/I/81e2YtCOKv...</td>\n      <td>item_weight</td>\n      <td>37 22 (uJel J0 wogoq mojaq ajeds &amp;38) (S3XL 7I...</td>\n      <td>0.0 gram, 2.0 gram</td>\n      <td>731432</td>\n      <td>1 kilogram</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>https://m.media-amazon.com/images/I/81RNsNEM1E...</td>\n      <td>item_weight</td>\n      <td># Glucon-D GREE' Reejucarzo0g Glucon-0 1 Insta...</td>\n      <td>0.0 gram, 10.0 gram, 170.0 milligram, 31.5 gra...</td>\n      <td>731432</td>\n      <td>200 gram</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>https://m.media-amazon.com/images/I/91prZeizZn...</td>\n      <td>item_weight</td>\n      <td># Glucon-D FREE' Regjucor20g Ipack Glucon-D 1 ...</td>\n      <td>20.0 gram, 6.0 gram</td>\n      <td>731432</td>\n      <td>200 gram</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>https://m.media-amazon.com/images/I/31EvJszFVf...</td>\n      <td>item_weight</td>\n      <td>iEE Glucon- 0 9.4 in 23 cm</td>\n      <td></td>\n      <td>731432</td>\n      <td>200 gram</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>https://m.media-amazon.com/images/I/61wzlucTRE...</td>\n      <td>item_volume</td>\n      <td>ONE SIZE FITS ALL STRETCHABLE AND BLACK FABRIC...</td>\n      <td>0.0 litre</td>\n      <td>252585</td>\n      <td>4.0 gallon</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>https://m.media-amazon.com/images/I/61sQ+qAKr4...</td>\n      <td>item_weight</td>\n      <td>925 Sterling Silver 18K Gold Plated 0.28 INCH ...</td>\n      <td>2.7 gram</td>\n      <td>299791</td>\n      <td>2.7 gram</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>https://m.media-amazon.com/images/I/81x77l2T5N...</td>\n      <td>item_weight</td>\n      <td>Crealig ; ( sustainable Valpednkatare 1 koryou...</td>\n      <td>112.0 gram, 28.0 gram</td>\n      <td>884560</td>\n      <td>112 gram</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>https://m.media-amazon.com/images/I/71nywfWZUw...</td>\n      <td>item_weight</td>\n      <td>9.1cm/3.58\" 36.8cm/ 14.48\" 481 13Ah 481 17BBAh...</td>\n      <td>44.0 kilogram</td>\n      <td>179080</td>\n      <td>4.1 kilogram</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>https://m.media-amazon.com/images/I/71nywfWZUw...</td>\n      <td>voltage</td>\n      <td>9.1cm/3.58\" 36.8cm/ 14.48\" 481 13Ah 481 17BBAh...</td>\n      <td>48.0 volt</td>\n      <td>179080</td>\n      <td>48.0 volt</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>https://m.media-amazon.com/images/I/51WsuKKAVr...</td>\n      <td>item_weight</td>\n      <td>Product Name Harvest Festival Doll Product Siz...</td>\n      <td>158.0 gram</td>\n      <td>866516</td>\n      <td>158.0 gram</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>https://m.media-amazon.com/images/I/61XGDKap+J...</td>\n      <td>item_weight</td>\n      <td>Hll Fbgt 885T</td>\n      <td></td>\n      <td>866516</td>\n      <td>158.0 gram</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>https://m.media-amazon.com/images/I/715vVcWJxG...</td>\n      <td>item_weight</td>\n      <td>Consistens NUVIDA NATURAL HEALTH CRANBERRY EXT...</td>\n      <td>5000.0 milligram, 90.0 gram</td>\n      <td>459516</td>\n      <td>5000 milligram</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>https://m.media-amazon.com/images/I/613v+2W4Uw...</td>\n      <td>item_weight</td>\n      <td>3 Jbapu3M'8ni *0-43 uedoulsbunoepibn Uilorjp3 ...</td>\n      <td></td>\n      <td>524635</td>\n      <td>18.55 gram</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nfrom fuzzywuzzy import fuzz\n\n# Function to calculate the prediction and the matching score\ndef predict_presence(entity_value, predicted_value, threshold=70):\n    # Calculate the fuzzy match ratio between the entity and the predicted value\n    match_score = fuzz.partial_ratio(entity_value.lower(), predicted_value.lower())\n    \n    # Predict presence based on the match score exceeding the threshold\n    presence = 1 if match_score >= threshold else 0\n    \n    return presence, match_score / 100  # Return as a probability (0 to 1)\n\n# Function to get the best match from the predicted values\ndef get_best_prediction(row, threshold=50):\n    predictions = row['predicted'].split(\", \")  # Split multiple predicted values\n    best_prediction = None\n    best_probability = 0\n    \n    for predicted_value in predictions:\n        presence, probability = predict_presence(row['entity_value'], predicted_value)\n        # Only consider values that meet the probability threshold and are better than the current best match\n        if probability > best_probability and probability > (threshold / 100):\n            best_prediction = predicted_value\n            best_probability = probability\n    \n    return best_prediction, best_probability\n\n# Apply the function to update the DataFrame\npred[['prediction', 'probability']] = pred.apply(\n    lambda row: get_best_prediction(row), axis=1, result_type='expand'\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T16:23:08.558567Z","iopub.execute_input":"2024-09-15T16:23:08.558920Z","iopub.status.idle":"2024-09-15T16:23:08.585067Z","shell.execute_reply.started":"2024-09-15T16:23:08.558888Z","shell.execute_reply":"2024-09-15T16:23:08.583891Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"# Show the DataFrame with predictions and probabilities\npred.head(30)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T16:23:09.313324Z","iopub.execute_input":"2024-09-15T16:23:09.313721Z","iopub.status.idle":"2024-09-15T16:23:09.337294Z","shell.execute_reply.started":"2024-09-15T16:23:09.313687Z","shell.execute_reply":"2024-09-15T16:23:09.336300Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"                                           image_link  entity_name  \\\n0   https://m.media-amazon.com/images/I/61I9XdN6OF...  item_weight   \n1   https://m.media-amazon.com/images/I/71gSRbyXmo...  item_volume   \n2   https://m.media-amazon.com/images/I/61BZ4zrjZX...  item_weight   \n3   https://m.media-amazon.com/images/I/612mrlqiI4...  item_weight   \n4   https://m.media-amazon.com/images/I/617Tl40LOX...  item_weight   \n5   https://m.media-amazon.com/images/I/61QsBSE7jg...  item_weight   \n6   https://m.media-amazon.com/images/I/81xsq6vf2q...  item_weight   \n7   https://m.media-amazon.com/images/I/71DiLRHeZd...  item_weight   \n8   https://m.media-amazon.com/images/I/91Cma3Rzse...  item_weight   \n9   https://m.media-amazon.com/images/I/71jBLhmTNl...  item_weight   \n10  https://m.media-amazon.com/images/I/81N73b5khV...  item_weight   \n11  https://m.media-amazon.com/images/I/61oMj2iXOu...  item_weight   \n12  https://m.media-amazon.com/images/I/91LPf6OjV9...  item_weight   \n13  https://m.media-amazon.com/images/I/81fOxWWWKY...  item_weight   \n14  https://m.media-amazon.com/images/I/81dzao1Ob4...  item_weight   \n15  https://m.media-amazon.com/images/I/91-iahVGED...  item_weight   \n16  https://m.media-amazon.com/images/I/81S2+GnYpT...  item_weight   \n17  https://m.media-amazon.com/images/I/81e2YtCOKv...  item_weight   \n18  https://m.media-amazon.com/images/I/81RNsNEM1E...  item_weight   \n19  https://m.media-amazon.com/images/I/91prZeizZn...  item_weight   \n20  https://m.media-amazon.com/images/I/31EvJszFVf...  item_weight   \n21  https://m.media-amazon.com/images/I/61wzlucTRE...  item_volume   \n22  https://m.media-amazon.com/images/I/61sQ+qAKr4...  item_weight   \n23  https://m.media-amazon.com/images/I/81x77l2T5N...  item_weight   \n24  https://m.media-amazon.com/images/I/71nywfWZUw...  item_weight   \n25  https://m.media-amazon.com/images/I/71nywfWZUw...      voltage   \n26  https://m.media-amazon.com/images/I/51WsuKKAVr...  item_weight   \n27  https://m.media-amazon.com/images/I/61XGDKap+J...  item_weight   \n28  https://m.media-amazon.com/images/I/715vVcWJxG...  item_weight   \n29  https://m.media-amazon.com/images/I/613v+2W4Uw...  item_weight   \n\n                                       extracted_text  \\\n0   PROPOS' NATUREJ INGREDIENT MENAGER MULTI-USAGE...   \n1   TLael=_ 767222 Xe RRIFIC; LEBENSMITTELECHT G D...   \n2   COMPOSITION Serving Size: Tablet (0.709 g) Eac...   \n3   conpositioN Serving Size: Tablet (0.709 g1 Eac...   \n4   Horbaach' HIGH StRENGTH PSYLLIUM HUSK PLANTAGO...   \n5   4JEEQJOH sue1ue1aba, 3 suebaA JOJ aqens - buiu...   \n6   HOrbaach Directions: For adults, take two (2) ...   \n7   VEGAN WHEAT FREE soy FREE OVATA DAIRY FREE FRE...   \n8   Horbaach V 100% HIGHEST QUALITY TO HGH GMP ) H...   \n9   NEW LOOK SAME TRUSTEDQUALITY OLD Horbaach NEW ...   \n10  GroBe Kapazilat; Tragialigkeit bis zu 30KG Das...   \n11  6XSt < 6XSt-GZ 6XSZ-GL 6XGl-0l 6x01 > UD 91 WD...   \n12  K Helps in osteoarthritis Kelps in digestion R...   \n13  (zo €s\"€) 6 00L JHOIJM LN Sasvasia SLVaWo)+ NO...   \n14  PACKAGING CHANGED FOOD FOR THE MODERN LIFE SUP...   \n15  X Loose packaging X Impurities added 7} Other ...   \n16  # Glucon-D FREE' Regutarz0g Ipack Glucon-D 1 I...   \n17  37 22 (uJel J0 wogoq mojaq ajeds &38) (S3XL 7I...   \n18  # Glucon-D GREE' Reejucarzo0g Glucon-0 1 Insta...   \n19  # Glucon-D FREE' Regjucor20g Ipack Glucon-D 1 ...   \n20                         iEE Glucon- 0 9.4 in 23 cm   \n21  ONE SIZE FITS ALL STRETCHABLE AND BLACK FABRIC...   \n22  925 Sterling Silver 18K Gold Plated 0.28 INCH ...   \n23  Crealig ; ( sustainable Valpednkatare 1 koryou...   \n24  9.1cm/3.58\" 36.8cm/ 14.48\" 481 13Ah 481 17BBAh...   \n25  9.1cm/3.58\" 36.8cm/ 14.48\" 481 13Ah 481 17BBAh...   \n26  Product Name Harvest Festival Doll Product Siz...   \n27                                      Hll Fbgt 885T   \n28  Consistens NUVIDA NATURAL HEALTH CRANBERRY EXT...   \n29  3 Jbapu3M'8ni *0-43 uedoulsbunoepibn Uilorjp3 ...   \n\n                                            predicted  group_id  \\\n0                                          500.0 gram    748919   \n1                                                        916768   \n2   0.04 gram, 0.09 gram, 0.2 gram, 0.51 gram, 0.7...    459516   \n3   0.2 gram, 0.51 gram, 0.709 gram, 10.0 milligra...    459516   \n4                                    1400.0 milligram    731432   \n5                                                        731432   \n6                                    1400.0 milligram    731432   \n7                                    1400.0 milligram    731432   \n8                                    1400.0 milligram    731432   \n9                   1400.0 milligram, 400.0 milligram    731432   \n10                                      30.0 kilogram    149159   \n11                                                       308856   \n12                                         100.0 gram    281678   \n13                                                       281678   \n14                               0.0 gram, 100.0 gram    281678   \n15                                                       281678   \n16                                           0.0 gram    731432   \n17                                 0.0 gram, 2.0 gram    731432   \n18  0.0 gram, 10.0 gram, 170.0 milligram, 31.5 gra...    731432   \n19                                20.0 gram, 6.0 gram    731432   \n20                                                       731432   \n21                                          0.0 litre    252585   \n22                                           2.7 gram    299791   \n23                              112.0 gram, 28.0 gram    884560   \n24                                      44.0 kilogram    179080   \n25                                          48.0 volt    179080   \n26                                         158.0 gram    866516   \n27                                                       866516   \n28                        5000.0 milligram, 90.0 gram    459516   \n29                                                       524635   \n\n                  entity_value        prediction  probability  \n0                   500.0 gram        500.0 gram         1.00  \n1                      1.0 cup               NaN         0.00  \n2                   0.709 gram        0.709 gram         1.00  \n3                   0.709 gram        0.709 gram         1.00  \n4               1400 milligram  1400.0 milligram         0.86  \n5               1400 milligram               NaN         0.00  \n6               1400 milligram  1400.0 milligram         0.86  \n7               1400 milligram  1400.0 milligram         0.86  \n8               1400 milligram  1400.0 milligram         0.86  \n9               1400 milligram  1400.0 milligram         0.86  \n10               30.0 kilogram     30.0 kilogram         1.00  \n11  10 kilogram to 15 kilogram               NaN         0.00  \n12                  3.53 ounce               NaN         0.00  \n13                  3.53 ounce               NaN         0.00  \n14                    53 ounce               NaN         0.00  \n15                    100 gram               NaN         0.00  \n16                    200 gram          0.0 gram         0.88  \n17                  1 kilogram               NaN         0.00  \n18                    200 gram          0.0 gram         0.88  \n19                    200 gram         20.0 gram         0.88  \n20                    200 gram               NaN         0.00  \n21                  4.0 gallon               NaN         0.00  \n22                    2.7 gram          2.7 gram         1.00  \n23                    112 gram        112.0 gram         0.75  \n24                4.1 kilogram     44.0 kilogram         0.92  \n25                   48.0 volt         48.0 volt         1.00  \n26                  158.0 gram        158.0 gram         1.00  \n27                  158.0 gram               NaN         0.00  \n28              5000 milligram  5000.0 milligram         0.93  \n29                  18.55 gram               NaN         0.00  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_link</th>\n      <th>entity_name</th>\n      <th>extracted_text</th>\n      <th>predicted</th>\n      <th>group_id</th>\n      <th>entity_value</th>\n      <th>prediction</th>\n      <th>probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://m.media-amazon.com/images/I/61I9XdN6OF...</td>\n      <td>item_weight</td>\n      <td>PROPOS' NATUREJ INGREDIENT MENAGER MULTI-USAGE...</td>\n      <td>500.0 gram</td>\n      <td>748919</td>\n      <td>500.0 gram</td>\n      <td>500.0 gram</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://m.media-amazon.com/images/I/71gSRbyXmo...</td>\n      <td>item_volume</td>\n      <td>TLael=_ 767222 Xe RRIFIC; LEBENSMITTELECHT G D...</td>\n      <td></td>\n      <td>916768</td>\n      <td>1.0 cup</td>\n      <td>NaN</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://m.media-amazon.com/images/I/61BZ4zrjZX...</td>\n      <td>item_weight</td>\n      <td>COMPOSITION Serving Size: Tablet (0.709 g) Eac...</td>\n      <td>0.04 gram, 0.09 gram, 0.2 gram, 0.51 gram, 0.7...</td>\n      <td>459516</td>\n      <td>0.709 gram</td>\n      <td>0.709 gram</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://m.media-amazon.com/images/I/612mrlqiI4...</td>\n      <td>item_weight</td>\n      <td>conpositioN Serving Size: Tablet (0.709 g1 Eac...</td>\n      <td>0.2 gram, 0.51 gram, 0.709 gram, 10.0 milligra...</td>\n      <td>459516</td>\n      <td>0.709 gram</td>\n      <td>0.709 gram</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://m.media-amazon.com/images/I/617Tl40LOX...</td>\n      <td>item_weight</td>\n      <td>Horbaach' HIGH StRENGTH PSYLLIUM HUSK PLANTAGO...</td>\n      <td>1400.0 milligram</td>\n      <td>731432</td>\n      <td>1400 milligram</td>\n      <td>1400.0 milligram</td>\n      <td>0.86</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>https://m.media-amazon.com/images/I/61QsBSE7jg...</td>\n      <td>item_weight</td>\n      <td>4JEEQJOH sue1ue1aba, 3 suebaA JOJ aqens - buiu...</td>\n      <td></td>\n      <td>731432</td>\n      <td>1400 milligram</td>\n      <td>NaN</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>https://m.media-amazon.com/images/I/81xsq6vf2q...</td>\n      <td>item_weight</td>\n      <td>HOrbaach Directions: For adults, take two (2) ...</td>\n      <td>1400.0 milligram</td>\n      <td>731432</td>\n      <td>1400 milligram</td>\n      <td>1400.0 milligram</td>\n      <td>0.86</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>https://m.media-amazon.com/images/I/71DiLRHeZd...</td>\n      <td>item_weight</td>\n      <td>VEGAN WHEAT FREE soy FREE OVATA DAIRY FREE FRE...</td>\n      <td>1400.0 milligram</td>\n      <td>731432</td>\n      <td>1400 milligram</td>\n      <td>1400.0 milligram</td>\n      <td>0.86</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>https://m.media-amazon.com/images/I/91Cma3Rzse...</td>\n      <td>item_weight</td>\n      <td>Horbaach V 100% HIGHEST QUALITY TO HGH GMP ) H...</td>\n      <td>1400.0 milligram</td>\n      <td>731432</td>\n      <td>1400 milligram</td>\n      <td>1400.0 milligram</td>\n      <td>0.86</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>https://m.media-amazon.com/images/I/71jBLhmTNl...</td>\n      <td>item_weight</td>\n      <td>NEW LOOK SAME TRUSTEDQUALITY OLD Horbaach NEW ...</td>\n      <td>1400.0 milligram, 400.0 milligram</td>\n      <td>731432</td>\n      <td>1400 milligram</td>\n      <td>1400.0 milligram</td>\n      <td>0.86</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>https://m.media-amazon.com/images/I/81N73b5khV...</td>\n      <td>item_weight</td>\n      <td>GroBe Kapazilat; Tragialigkeit bis zu 30KG Das...</td>\n      <td>30.0 kilogram</td>\n      <td>149159</td>\n      <td>30.0 kilogram</td>\n      <td>30.0 kilogram</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>https://m.media-amazon.com/images/I/61oMj2iXOu...</td>\n      <td>item_weight</td>\n      <td>6XSt &lt; 6XSt-GZ 6XSZ-GL 6XGl-0l 6x01 &gt; UD 91 WD...</td>\n      <td></td>\n      <td>308856</td>\n      <td>10 kilogram to 15 kilogram</td>\n      <td>NaN</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>https://m.media-amazon.com/images/I/91LPf6OjV9...</td>\n      <td>item_weight</td>\n      <td>K Helps in osteoarthritis Kelps in digestion R...</td>\n      <td>100.0 gram</td>\n      <td>281678</td>\n      <td>3.53 ounce</td>\n      <td>NaN</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>https://m.media-amazon.com/images/I/81fOxWWWKY...</td>\n      <td>item_weight</td>\n      <td>(zo €s\"€) 6 00L JHOIJM LN Sasvasia SLVaWo)+ NO...</td>\n      <td></td>\n      <td>281678</td>\n      <td>3.53 ounce</td>\n      <td>NaN</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>https://m.media-amazon.com/images/I/81dzao1Ob4...</td>\n      <td>item_weight</td>\n      <td>PACKAGING CHANGED FOOD FOR THE MODERN LIFE SUP...</td>\n      <td>0.0 gram, 100.0 gram</td>\n      <td>281678</td>\n      <td>53 ounce</td>\n      <td>NaN</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>https://m.media-amazon.com/images/I/91-iahVGED...</td>\n      <td>item_weight</td>\n      <td>X Loose packaging X Impurities added 7} Other ...</td>\n      <td></td>\n      <td>281678</td>\n      <td>100 gram</td>\n      <td>NaN</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>https://m.media-amazon.com/images/I/81S2+GnYpT...</td>\n      <td>item_weight</td>\n      <td># Glucon-D FREE' Regutarz0g Ipack Glucon-D 1 I...</td>\n      <td>0.0 gram</td>\n      <td>731432</td>\n      <td>200 gram</td>\n      <td>0.0 gram</td>\n      <td>0.88</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>https://m.media-amazon.com/images/I/81e2YtCOKv...</td>\n      <td>item_weight</td>\n      <td>37 22 (uJel J0 wogoq mojaq ajeds &amp;38) (S3XL 7I...</td>\n      <td>0.0 gram, 2.0 gram</td>\n      <td>731432</td>\n      <td>1 kilogram</td>\n      <td>NaN</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>https://m.media-amazon.com/images/I/81RNsNEM1E...</td>\n      <td>item_weight</td>\n      <td># Glucon-D GREE' Reejucarzo0g Glucon-0 1 Insta...</td>\n      <td>0.0 gram, 10.0 gram, 170.0 milligram, 31.5 gra...</td>\n      <td>731432</td>\n      <td>200 gram</td>\n      <td>0.0 gram</td>\n      <td>0.88</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>https://m.media-amazon.com/images/I/91prZeizZn...</td>\n      <td>item_weight</td>\n      <td># Glucon-D FREE' Regjucor20g Ipack Glucon-D 1 ...</td>\n      <td>20.0 gram, 6.0 gram</td>\n      <td>731432</td>\n      <td>200 gram</td>\n      <td>20.0 gram</td>\n      <td>0.88</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>https://m.media-amazon.com/images/I/31EvJszFVf...</td>\n      <td>item_weight</td>\n      <td>iEE Glucon- 0 9.4 in 23 cm</td>\n      <td></td>\n      <td>731432</td>\n      <td>200 gram</td>\n      <td>NaN</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>https://m.media-amazon.com/images/I/61wzlucTRE...</td>\n      <td>item_volume</td>\n      <td>ONE SIZE FITS ALL STRETCHABLE AND BLACK FABRIC...</td>\n      <td>0.0 litre</td>\n      <td>252585</td>\n      <td>4.0 gallon</td>\n      <td>NaN</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>https://m.media-amazon.com/images/I/61sQ+qAKr4...</td>\n      <td>item_weight</td>\n      <td>925 Sterling Silver 18K Gold Plated 0.28 INCH ...</td>\n      <td>2.7 gram</td>\n      <td>299791</td>\n      <td>2.7 gram</td>\n      <td>2.7 gram</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>https://m.media-amazon.com/images/I/81x77l2T5N...</td>\n      <td>item_weight</td>\n      <td>Crealig ; ( sustainable Valpednkatare 1 koryou...</td>\n      <td>112.0 gram, 28.0 gram</td>\n      <td>884560</td>\n      <td>112 gram</td>\n      <td>112.0 gram</td>\n      <td>0.75</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>https://m.media-amazon.com/images/I/71nywfWZUw...</td>\n      <td>item_weight</td>\n      <td>9.1cm/3.58\" 36.8cm/ 14.48\" 481 13Ah 481 17BBAh...</td>\n      <td>44.0 kilogram</td>\n      <td>179080</td>\n      <td>4.1 kilogram</td>\n      <td>44.0 kilogram</td>\n      <td>0.92</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>https://m.media-amazon.com/images/I/71nywfWZUw...</td>\n      <td>voltage</td>\n      <td>9.1cm/3.58\" 36.8cm/ 14.48\" 481 13Ah 481 17BBAh...</td>\n      <td>48.0 volt</td>\n      <td>179080</td>\n      <td>48.0 volt</td>\n      <td>48.0 volt</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>https://m.media-amazon.com/images/I/51WsuKKAVr...</td>\n      <td>item_weight</td>\n      <td>Product Name Harvest Festival Doll Product Siz...</td>\n      <td>158.0 gram</td>\n      <td>866516</td>\n      <td>158.0 gram</td>\n      <td>158.0 gram</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>https://m.media-amazon.com/images/I/61XGDKap+J...</td>\n      <td>item_weight</td>\n      <td>Hll Fbgt 885T</td>\n      <td></td>\n      <td>866516</td>\n      <td>158.0 gram</td>\n      <td>NaN</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>https://m.media-amazon.com/images/I/715vVcWJxG...</td>\n      <td>item_weight</td>\n      <td>Consistens NUVIDA NATURAL HEALTH CRANBERRY EXT...</td>\n      <td>5000.0 milligram, 90.0 gram</td>\n      <td>459516</td>\n      <td>5000 milligram</td>\n      <td>5000.0 milligram</td>\n      <td>0.93</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>https://m.media-amazon.com/images/I/613v+2W4Uw...</td>\n      <td>item_weight</td>\n      <td>3 Jbapu3M'8ni *0-43 uedoulsbunoepibn Uilorjp3 ...</td>\n      <td></td>\n      <td>524635</td>\n      <td>18.55 gram</td>\n      <td>NaN</td>\n      <td>0.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"markdown","source":"for black n white conversion ","metadata":{}},{"cell_type":"code","source":"\"\"\"import requests\nfrom PIL import Image\nfrom io import BytesIO\n\ndef download_image_from_url(image_url, save_path):\n    try:\n        # Send a GET request to the URL\n        response = requests.get(image_url)\n        \n        # Raise an exception if the request was unsuccessful\n        response.raise_for_status()\n        \n        # Open the image from the fetched bytes\n        image = Image.open(BytesIO(response.content))\n        \n        # Convert the image to black and white (grayscale)\n        image_bw = image.convert('L')\n        \n        # Save the black and white image to the specified path\n        image_bw.save(save_path)\n        \n        print(f\"Black and white image successfully saved at {save_path}\")\n        \n    except requests.exceptions.RequestException as e:\n        print(f\"Error downloading the image: {e}\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n# Example usage\nimage_url = 'https://m.media-amazon.com/images/I/61I9XdN6OFL.jpg'\nsave_path = 'downloaded_image_bw.jpg'\n\ndownload_image_from_url(image_url, save_path)\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# For multiple images extraction using links in the csv file","metadata":{}},{"cell_type":"code","source":"\"\"\"\"\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport os\n\ndef download_images_from_df(df, save_directory, url_column='image link', limit=1000):\n    try:\n        # Create the directory to save images if it doesn't exist\n        if not os.path.exists(save_directory):\n            os.makedirs(save_directory)\n        \n        # Ensure the column name is correct\n        if url_column not in df.columns:\n            print(f\"Column '{url_column}' not found. Available columns: {df.columns.tolist()}\")\n            return\n        \n        # Counter to track how many images have been downloaded\n        download_count = 0\n        \n        # Loop through the URLs in the specified column, but stop after reaching the limit\n        for index, row in df.iterrows():\n            if download_count >= limit:\n                print(f\"Reached the download limit of {limit} images.\")\n                break\n            \n            image_url = row[url_column]\n            try:\n                # Ensure the URL is valid and not NaN or empty\n                if pd.isna(image_url) or not isinstance(image_url, str):\n                    continue\n\n                # Send a GET request to the URL\n                response = requests.get(image_url)\n                response.raise_for_status()\n                \n                # Open the image from the fetched bytes\n                image = Image.open(BytesIO(response.content))\n                \n                # Construct the save path using index to make each filename unique\n                image_save_path = os.path.join(save_directory, f'image_{index + 1}.jpg')\n                \n                # Save the image\n                image.save(image_save_path)\n                \n                # Increment the download count and show only the count\n                download_count += 1\n                print(f\"Total downloaded: {download_count}\", end='\\r')  # Updates the same line\n                \n            except requests.exceptions.RequestException:\n                continue\n            except Exception:\n                continue\n    \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n# Example usage\nsave_directory = 'downloaded_images'  # Directory to save downloaded images\n\n# Assuming your DataFrame is already loaded in 'df'\n# Control the limit of downloads here\ndownload_images_from_df(df, save_directory, url_column='image_link', limit=10)\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\"\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport os\nimport pandas as pd\n\ndef download_images_from_df(df, save_directory, url_column='image link', limit=1000):\n    try:\n        # Create the directory to save images if it doesn't exist\n        if not os.path.exists(save_directory):\n            os.makedirs(save_directory)\n        \n        # Ensure the column name is correct\n        if url_column not in df.columns:\n            print(f\"Column '{url_column}' not found. Available columns: {df.columns.tolist()}\")\n            return\n        \n        # Counter to track how many images have been downloaded\n        download_count = 0\n        \n        # Loop through the URLs in the specified column, but stop after reaching the limit\n        for index, row in df.iterrows():\n            if download_count >= limit:\n                print(f\"Reached the download limit of {limit} images.\")\n                break\n            \n            image_url = row[url_column]\n            try:\n                # Ensure the URL is valid and not NaN or empty\n                if pd.isna(image_url) or not isinstance(image_url, str):\n                    continue\n\n                # Send a GET request to the URL\n                response = requests.get(image_url)\n                response.raise_for_status()\n                \n                # Open the image from the fetched bytes\n                image = Image.open(BytesIO(response.content))\n                \n                # Construct the save path using index to make each filename unique\n                image_save_path = os.path.join(save_directory, f'image_{index + 1}.jpg')\n                \n                # Save the image\n                image.save(image_save_path)\n                \n                # Increment the download count and show only the count\n                download_count += 1\n                print(f\"Total downloaded: {download_count}\", end='\\r')  # Updates the same line\n                \n            except requests.exceptions.RequestException:\n                continue\n            except Exception:\n                continue\n    \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n# Example usage\nsave_directory = 'downloaded_images'  # Directory to save downloaded images\n\n# Assuming your DataFrame is already loaded in 'df'\n# Control the limit of downloads here\ndownload_images_from_df(df, save_directory, url_column='image_link', limit=100)\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test image extraction","metadata":{}},{"cell_type":"code","source":"\"\"\"\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport os\nimport pandas as pd\n\ndef download_images_from_df(df, save_directory, url_column='image link', limit=1000):\n    try:\n        # Create the directory to save images if it doesn't exist\n        if not os.path.exists(save_directory):\n            os.makedirs(save_directory)\n        \n        # Ensure the column name is correct\n        if url_column not in df.columns:\n            print(f\"Column '{url_column}' not found. Available columns: {df.columns.tolist()}\")\n            return\n        \n        # Counter to track how many images have been downloaded\n        download_count = 0\n        \n        # Loop through the URLs in the specified column, but stop after reaching the limit\n        for index, row in df.iterrows():\n            if download_count >= limit:\n                print(f\"Reached the download limit of {limit} images.\")\n                break\n            \n            image_url = row[url_column]\n            try:\n                # Ensure the URL is valid and not NaN or empty\n                if pd.isna(image_url) or not isinstance(image_url, str):\n                    continue\n\n                # Send a GET request to the URL\n                response = requests.get(image_url)\n                response.raise_for_status()\n                \n                # Open the image from the fetched bytes\n                image = Image.open(BytesIO(response.content))\n                \n                # Construct the save path using index to make each filename unique\n                image_save_path = os.path.join(save_directory, f'image_{index + 1}.jpg')\n                \n                # Save the image\n                image.save(image_save_path)\n                \n                # Increment the download count and show only the count\n                download_count += 1\n                print(f\"Total downloaded: {download_count}\", end='\\r')  # Updates the same line\n                \n            except requests.exceptions.RequestException:\n                continue\n            except Exception:\n                continue\n    \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n# Example usage\nsave_directory = 'downloaded_test_images'  # Directory to save downloaded images\n\n# Assuming your DataFrame is already loaded in 'df'\n# Control the limit of downloads here\ndownload_images_from_df(df, save_directory, url_column='image_link', limit=100)\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Single image text extraction","metadata":{}},{"cell_type":"code","source":"\"\"\"\nimport easyocr\nimport pandas as pd\nfrom PIL import Image\nimport numpy as np\nimport requests\nfrom io import BytesIO\nimport re\n\n# Initialize the EasyOCR reader (CPU mode if GPU is not available or has low memory)\nreader = easyocr.Reader(['en'], gpu=True)\n\n# Function to download an image from a URL\ndef download_image(url):\n    response = requests.get(url)\n    img = Image.open(BytesIO(response.content))\n    return img\n\n# Function to rotate the image and check OCR text\ndef rotate_and_extract(image):\n    # Initialize variables to track the best result\n    best_text = \"\"\n    best_angle = 0\n    \n    # Try OCR on the original and rotated images\n    for angle in [0, 90, 180, 270]:\n        rotated_img = image.rotate(angle, expand=True)\n        \n        # Convert the rotated image to a numpy array for easyocr\n        rotated_img_np = np.array(rotated_img)\n        \n        # Extract text from rotated image\n        img_text = reader.readtext(rotated_img_np)\n        final_text = \" \".join([text for _, text, __ in img_text])\n        \n        if len(final_text.strip()) > len(best_text.strip()):  # Choose the best text based on length\n            best_text = final_text\n            best_angle = angle\n    \n    # Return the best text and angle\n    return best_text, best_angle\n\n# Function to check if the text contains weight in grams and extract the value\ndef extract_weight_in_grams(text):\n    # Regular expression pattern to match weight values in grams (e.g., 100g, 250 grams, 0.709g)\n    weight_pattern = r'(\\d+(\\.\\d+)?)\\s*(g|grams)'\n    \n    # Search the text for the pattern\n    match = re.search(weight_pattern, text.lower())  # Convert text to lowercase for better matching\n    \n    if match:\n        # If a match is found, extract the numerical part (group 1) and return it as a float\n        return float(match.group(1))\n    else:\n        return \"Weight in grams not found\"\n\n# Function to process each image URL\ndef process_image_url(image_url):\n    # Download and process the image\n    image = download_image(image_url)\n    final_text, best_angle = rotate_and_extract(image)\n    \n    # Extract weight in grams from the final text\n    weight = extract_weight_in_grams(final_text)\n    \n    return final_text, best_angle, weight\n\n# Load the CSV file\ncsv_file_path = '/kaggle/input/amazon-dataset/student_resource 3/dataset/train.csv'\ndf = pd.read_csv(csv_file_path)\n\n# Iterate through each image URL in the CSV file\nfor index, row in df.iterrows():\n    image_url = row['image_link']  # Assuming the CSV column is named 'image_url'\n    print(f\"Processing image from URL: {image_url}\")\n    \n    final_text, best_angle, weight = process_image_url(image_url)\n    \n    # Print results for each image\n    print(f\"Extracted Text (Best angle {best_angle} degrees): {final_text}\")\n    print(f\"Detected weight in grams: {weight} grams\")\n    print(\"--------------------------------------------------\")\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nimport easyocr\nfrom PIL import Image\nimport re\nimport numpy as np\n\n# Initialize the EasyOCR reader (CPU mode if GPU is not available or has low memory)\nreader = easyocr.Reader(['en'], gpu=True)\n\n# Function to rotate the image and check OCR text\ndef rotate_and_extract(image):\n    # Initialize variables to track the best result\n    best_text = \"\"\n    best_angle = 0\n    \n    # Try OCR on the original and rotated images\n    for angle in [0, 90, 180, 270]:\n        rotated_img = image.rotate(angle, expand=True)\n        \n        # Convert the rotated image to a numpy array for easyocr\n        rotated_img_np = np.array(rotated_img)\n        \n        # Extract text from rotated image\n        img_text = reader.readtext(rotated_img_np)\n        final_text = \" \".join([text for _, text, __ in img_text])\n        \n        if len(final_text.strip()) > len(best_text.strip()):  # Choose the best text based on length\n            best_text = final_text\n            best_angle = angle\n    \n    # Return the best text and angle\n    return best_text, best_angle\n\n# Function to check if the text contains weight in grams and extract the value\ndef extract_weight_in_grams(text):\n    # Regular expression pattern to match weight values in grams (e.g., 100g, 250 grams, 0.709g)\n    weight_pattern = r'(\\d+(\\.\\d+)?)\\s*(g|grams)'\n    \n    # Search the text for the pattern\n    match = re.search(weight_pattern, text.lower())  # Convert text to lowercase for better matching\n    \n    if match:\n        # If a match is found, extract the numerical part (group 1) and return it as a float\n        return float(match.group(1))\n    else:\n        return \"Weight in grams not found\"\n\n# Example of using the provided black-and-white image directly\n# Load the image (assuming it's already black-and-white)\nimage = Image.open('/kaggle/working/downloaded_image_bw.jpg')\n\n# Rotate image if needed and get the corrected text\nfinal_text, best_angle = rotate_and_extract(image)\n\n# Print the extracted text and best angle\nprint(f\"Extracted Text (Best angle {best_angle} degrees):\", final_text)\n\n# Extract weight in grams from the final text\nweight = extract_weight_in_grams(final_text)\n\n# Output the result\nprint(\"Detected weight in grams:\", weight, \"grams\")\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image folder text extraction","metadata":{}},{"cell_type":"code","source":"\"\"\"\nimport easyocr\nfrom PIL import Image\nimport re\nimport os\n\n# Initialize the EasyOCR reader (CPU mode if GPU is not available or has low memory)\nreader = easyocr.Reader(['en'], gpu=False)\n\n# Function to rotate the image and check OCR text\ndef rotate_and_extract(image_path):\n    # Open the image\n    img = Image.open(image_path)\n    \n    # Initialize variables to track the best result\n    best_text = \"\"\n    best_angle = 0\n    \n    # Try OCR on the original and rotated images\n    for angle in [0, 90, 180, 270]:\n        rotated_img = img.rotate(angle, expand=True)\n        rotated_img_path = f\"/kaggle/working/downloaded_images2/rotated_{angle}.jpg\"\n        rotated_img.save(rotated_img_path)\n        \n        # Extract text from rotated image\n        img_text = reader.readtext(rotated_img_path)\n        final_text = \" \".join([text for _, text, __ in img_text])\n        \n        if len(final_text.strip()) > len(best_text.strip()):  # Choose the best text based on length\n            best_text = final_text\n            best_angle = angle\n    \n    # Return the best text and angle\n    return best_text, best_angle\n\n# Function to check if the text contains weight in grams and extract the value\ndef extract_weight_in_grams(text):\n    # Regular expression pattern to match weight values in grams (e.g., 100g, 250 grams, 0.709g)\n    weight_pattern = r'(\\d+(\\.\\d+)?)\\s*(g|grams)'\n    \n    # Search the text for the pattern\n    match = re.search(weight_pattern, text.lower())  # Convert text to lowercase for better matching\n    \n    if match:\n        # If a match is found, extract the numerical part (group 1) and return it as a float\n        return float(match.group(1))\n    else:\n        return \"Weight in grams not found\"\n\n# Directory containing images\nimage_directory = '/kaggle/working/downloaded_images2/'\n\n# List all image files in the directory\nimage_files = [f for f in os.listdir(image_directory) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n\n# Process each image file\nfor image_file in image_files:\n    image_path = os.path.join(image_direcatory, image_file)\n    \n    # Rotate image if needed and get the corrected text\n    final_text, best_angle = rotate_and_extract(image_path)\n    \n    # Print the extracted text and best angle\n    print(f\"\\nImage: {image_file}\")\n    print(f\"Extracted Text (Best angle {best_angle} degrees):\", final_text)\n    \n    # Extract weight in grams from the final text\n    weight = extract_weight_in_grams(final_text)\n    \n    # Output the result\n    print(\"Detected weight in grams:\", weight, \"grams\")\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"import easyocr\nfrom PIL import Image\nimport re\nimport os\n\n# Initialize the EasyOCR reader (CPU mode if GPU is not available or has low memory)\nreader = easyocr.Reader(['en'], gpu=False)\n\n# Function to rotate the image and check OCR text\ndef rotate_and_extract(image_path):\n    # Open the image\n    img = Image.open(image_path)\n    \n    # Initialize variables to track the best result\n    best_text = \"\"\n    best_angle = 0\n    \n    # Try OCR on the original and rotated images\n    for angle in [0, 90, 180, 270]:\n        rotated_img = img.rotate(angle, expand=True)\n        rotated_img_path = f\"/kaggle/working/downloaded_images2/rotated_{angle}.jpg\"\n        rotated_img.save(rotated_img_path)\n        \n        # Extract text from rotated image\n        img_text = reader.readtext(rotated_img_path)\n        final_text = \" \".join([text for _, text, __ in img_text])\n        \n        if len(final_text.strip()) > len(best_text.strip()):  # Choose the best text based on length\n            best_text = final_text\n            best_angle = angle\n    \n    # Return the best text and angle\n    return best_text, best_angle\n\n# Function to extract weight in grams\ndef extract_weight_in_grams(text):\n    weight_pattern = r'(\\d+(\\.\\d+)?)\\s*(g|grams)'\n    match = re.search(weight_pattern, text.lower())\n    if match:\n        return float(match.group(1)), \"grams\"\n    else:\n        return \"Weight in grams not found\"\n\n# Function to extract weight in milligrams (mg)\ndef extract_weight_in_milligrams(text):\n    milligram_pattern = r'(\\d+(\\.\\d+)?)\\s*(mg|milligrams?)'\n    match = re.search(milligram_pattern, text.lower())\n    if match:\n        return float(match.group(1)), \"milligrams\"\n    else:\n        return \"Weight in milligrams not found\"\n\n# Function to extract weight in kilograms (kg)\ndef extract_weight_in_kilograms(text):\n    kilogram_pattern = r'(\\d+(\\.\\d+)?)\\s*(kg|kilograms?)'\n    match = re.search(kilogram_pattern, text.lower())\n    if match:\n        return float(match.group(1)), \"kilograms\"\n    else:\n        return \"Weight in kilograms not found\"\n\n# Function to extract weight in ounces (oz)\ndef extract_weight_in_ounces(text):\n    ounce_pattern = r'(\\d+(\\.\\d+)?)\\s*(oz|ounce|ounces?)'\n    match = re.search(ounce_pattern, text.lower())\n    if match:\n        return float(match.group(1)), \"ounces\"\n    else:\n        return \"Weight in ounces not found\"\n\n# Function to extract force in newtons (N)\ndef extract_force_in_newtons(text):\n    newton_pattern = r'(\\d+(\\.\\d+)?)\\s*(n|newtons?)'\n    match = re.search(newton_pattern, text.lower())\n    if match:\n        return float(match.group(1)), \"newtons\"\n    else:\n        return \"Force in newtons not found\"\n\n# Function to extract volume in liters/milliliters\ndef extract_volume_in_liters(text):\n    volume_pattern = r'(\\d+(\\.\\d+)?)\\s*(l|liter|liters|ml|milliliters|milliliter)'\n    match = re.search(volume_pattern, text.lower())\n    if match:\n        value = float(match.group(1))\n        unit = match.group(3)\n        if unit in ['ml', 'milliliters', 'milliliter']:\n            value /= 1000.0  # Convert milliliters to liters\n        return value, \"liters\"\n    else:\n        return \"Volume in liters/milliliters not found\"\n\n# Function to extract wattage in W\ndef extract_wattage(text):\n    wattage_pattern = r'(\\d+(\\.\\d+)?)\\s*(w|watt|watts)'\n    match = re.search(wattage_pattern, text.lower())\n    if match:\n        return float(match.group(1)), \"watts\"\n    else:\n        return \"Wattage not found\"\n\n# Function to extract voltage in V\ndef extract_voltage(text):\n    voltage_pattern = r'(\\d+(\\.\\d+)?)\\s*(v|volt|volts|voltage)'\n    match = re.search(voltage_pattern, text.lower())\n    if match:\n        return float(match.group(1)), \"volts\"\n    else:\n        return \"Voltage not found\"\n\n# Function to extract length in inches, cm, meters, or feet\ndef extract_length(text):\n    length_pattern = r'(\\d+(\\.\\d+)?)\\s*(in|inch|cm|centimeter|meters?|m|ft|foot|feet)'\n    match = re.search(length_pattern, text.lower())\n    if match:\n        value = float(match.group(1))\n        unit = match.group(3)\n        return value, unit\n    else:\n        return \"Length not found\"\n\n# Function to extract weight in pounds (lbs)\ndef extract_weight_in_lbs(text):\n    pound_pattern = r'(\\d+(\\.\\d+)?)\\s*(lbs|pounds?)'\n    match = re.search(pound_pattern, text.lower())\n    if match:\n        return float(match.group(1)), \"lbs\"\n    else:\n        return \"Weight in pounds not found\"\n\n# Directory containing images\nimage_directory = '/kaggle/working/downloaded_images2/'\n\n# List all image files in the directory\nimage_files = [f for f in os.listdir(image_directory) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n\n# Process each image file\nfor image_file in image_files:\n    image_path = os.path.join(image_directory, image_file)\n    \n    # Rotate image if needed and get the corrected text\n    final_text, best_angle = rotate_and_extract(image_path)\n    \n    # Print the extracted text and best angle\n    print(f\"\\nImage: {image_file}\")\n    print(f\"Extracted Text (Best angle {best_angle} degrees):\", final_text)\n    \n    # Extract weight in grams\n    weight = extract_weight_in_grams(final_text)\n    print(\"Detected weight in grams:\", weight)\n    \n    # Extract weight in milligrams\n    weight_mg = extract_weight_in_milligrams(final_text)\n    print(\"Detected weight in milligrams:\", weight_mg)\n    \n    # Extract weight in kilograms\n    weight_kg = extract_weight_in_kilograms(final_text)\n    print(\"Detected weight in kilograms:\", weight_kg)\n    \n    # Extract weight in ounces\n    weight_oz = extract_weight_in_ounces(final_text)\n    print(\"Detected weight in ounces:\", weight_oz)\n    \n    # Extract force in newtons\n    force_n = extract_force_in_newtons(final_text)\n    print(\"Detected force in newtons:\", force_n)\n    \n    # Extract volume in liters\n    volume = extract_volume_in_liters(final_text)\n    if isinstance(volume, tuple):\n        print(f\"Detected volume: {volume[0]} {volume[1]}\")\n    else:\n        print(volume)\n    \n    # Extract wattage\n    wattage = extract_wattage(final_text)\n    if isinstance(wattage, tuple):\n        print(f\"Detected wattage: {wattage[0]} {wattage[1]}\")\n    else:\n        print(wattage)\n    \n    # Extract voltage\n    voltage = extract_voltage(final_text)\n    if isinstance(voltage, tuple):\n        print(f\"Detected voltage: {voltage[0]} {voltage[1]}\")\n    else:\n        print(voltage)\n    \n    # Extract length\n    length = extract_length(final_text)\n    if isinstance(length, tuple):\n        print(f\"Detected length: {length[0]} {length[1]}\")\n    else:\n        print(length)\n    \n    # Extract weight in pounds (lbs)\n    weight_lbs = extract_weight_in_lbs(final_text)\n    if isinstance(weight_lbs, tuple):\n        print(f\"Detected weight in pounds: {weight_lbs[0]} {weight_lbs[1]}\")\n    else:\n        print(weight_lbs)\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text extraction for training","metadata":{}},{"cell_type":"code","source":"\"\"\"\nimport easyocr\nfrom PIL import Image\nimport re\nimport os\nimport pandas as pd\nimport requests\nfrom io import BytesIO\nimport concurrent.futures\nimport numpy as np\n\n# Initialize the EasyOCR reader (using GPU if available)\nreader = easyocr.Reader(['en'], gpu=True)\n\n# Function to download an image or load it from the local directory\ndef download_image(image_url, save_directory):\n    try:\n        response = requests.get(image_url, timeout=5)\n        img = Image.open(BytesIO(response.content))\n        image_name = os.path.basename(image_url)\n        save_path = os.path.join(save_directory, image_name)\n        img.save(save_path)\n        return save_path\n    except Exception as e:\n        print(f\"Error downloading image {image_url}: {e}\")\n        return None\n\n# Function to extract text from an image without rotation\ndef extract_text_from_image(image_path):\n    try:\n        img = Image.open(image_path)\n\n        # Resize image to a smaller size for faster processing (reduce by half)\n        img = img.resize((img.width // 2, img.height // 2))\n\n        # Convert image to NumPy array for OCR\n        img_np = np.array(img)\n\n        # Extract text using OCR\n        img_text = reader.readtext(img_np)\n        final_text = \" \".join([text for _, text, __ in img_text])\n\n        return final_text\n    except OSError as e:\n        print(f\"Error processing image {image_path}: {e}\")\n        return \"\"\n\n# Function to search for a specific entity using regular expressions\ndef search_entity(text, entity):\n    patterns = {\n        'grams': r'(\\d+(\\.\\d+)?)\\s*(g|grams?)',\n        'milligrams': r'(\\d+(\\.\\d+)?)\\s*(mg|milligrams?)',\n        'kilograms': r'(\\d+(\\.\\d+)?)\\s*(kg|kilograms?)',\n        'ounces': r'(\\d+(\\.\\d+)?)\\s*(oz|ounces?)',\n        'newtons': r'(\\d+(\\.\\d+)?)\\s*(n|newtons?)',\n        'liters': r'(\\d+(\\.\\d+)?)\\s*(l|liters?|ml|milliliters?)',\n        'watts': r'(\\d+(\\.\\d+)?)\\s*(w|watts?)',\n        'volts': r'(\\d+(\\.\\d+)?)\\s*(v|volts?)',\n        'inches': r'(\\d+(\\.\\d+)?)\\s*(in|inches?)',\n        'centimeters': r'(\\d+(\\.\\d+)?)\\s*(cm|centimeters?)',\n        'meters': r'(\\d+(\\.\\d+)?)\\s*(m|meters?)',\n        'feet': r'(\\d+(\\.\\d+)?)\\s*(ft|feet?)',\n        'pounds': r'(\\d+(\\.\\d+)?)\\s*(lbs|pounds?)',\n    }\n\n    pattern = patterns.get(entity.lower())\n    if pattern:\n        match = re.search(pattern, text.lower())\n        if match:\n            return f\"{match.group(1)} {entity.lower()}\", match.group(1)\n    \n    return f\"{entity} not found\", None\n\n# Function to process a single row of the DataFrame\ndef process_row(row, image_directory):\n    image_url = row['image_link']\n    entity_name = row['entity_name']\n\n    # Download the image if it's a URL\n    if image_url.startswith('http'):\n        image_path = download_image(image_url, image_directory)\n    else:\n        image_path = os.path.join(image_directory, image_url)\n\n    if not image_path:\n        return \"Download Error\", \"\", None\n\n    # Extract text without rotation\n    final_text = extract_text_from_image(image_path)\n\n    if not final_text:\n        return \"Image Error\", \"\", None\n\n    # Search for the entity in the extracted text\n    entity_result, entity_value = search_entity(final_text, entity_name)\n\n    return entity_result, final_text, entity_value\n\n# Directory to save downloaded images\nimage_directory = '/kaggle/working/downloaded_images/'\n\n# Create directory if it doesn't exist\nif not os.path.exists(image_directory):\n    os.makedirs(image_directory)\n\n# Load the CSV containing image URLs and entity names\ncsv_path = '/kaggle/input/amazon-dataset/student_resource 3/dataset/train.csv'\ndf = pd.read_csv(csv_path)\n\n# Process a limited number of rows from the DataFrame\nlimit = 1000  # Set limit on the number of images to process\ndf_limited = df.head(limit)\n\n# Process the rows in parallel using ThreadPoolExecutor\nwith concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n    results = list(executor.map(lambda row: process_row(row, image_directory), [row for _, row in df_limited.iterrows()]))\n\n# Unpack results\npredictions, extracted_texts, entity_values = zip(*results)\n\n# Add the predictions, extracted texts, and entity values to the DataFrame\ndf_limited['extracted_text'] = extracted_texts\ndf_limited['prediction'] = predictions\ndf_limited['entity_value'] = entity_values\n\n# Save the updated DataFrame to a CSV file\noutput_csv_path = '/kaggle/working/entity_predictions_with_text_and_value.csv'\ndf_limited.to_csv(output_csv_path, index=False)\n\nprint(f\"Results saved to {output_csv_path}\")\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text extraction for testing","metadata":{}},{"cell_type":"code","source":"\"\"\"\nimport easyocr\nfrom PIL import Image\nimport re\nimport os\nimport pandas as pd\nimport requests\nfrom io import BytesIO\nimport concurrent.futures\nimport numpy as np\n\n# Initialize the EasyOCR reader (using GPU if available)\nreader = easyocr.Reader(['en'], gpu=True)\n\n# Function to download an image or load it from the local directory\ndef download_image(image_url, save_directory):\n    try:\n        response = requests.get(image_url, timeout=5)\n        response.raise_for_status()  # Ensure we notice bad responses\n        img = Image.open(BytesIO(response.content))\n        image_name = os.path.basename(image_url)\n        save_path = os.path.join(save_directory, image_name)\n        img.save(save_path)\n        return save_path\n    except Exception as e:\n        print(f\"Error downloading image {image_url}: {e}\")\n        return None\n\n# Function to extract text from an image without rotation\ndef extract_text_from_image(image_path):\n    try:\n        img = Image.open(image_path)\n\n        # Resize image to a smaller size for faster processing (reduce by half)\n        img = img.resize((img.width // 2, img.height // 2))\n\n        # Convert image to NumPy array for OCR\n        img_np = np.array(img)\n\n        # Extract text using OCR\n        img_text = reader.readtext(img_np)\n        final_text = \" \".join([text for _, text, __ in img_text])\n\n        return final_text\n    except OSError as e:\n        print(f\"Error processing image {image_path}: {e}\")\n        return \"\"\n\n# Function to search for a specific entity using regular expressions\ndef search_entity(text, entity):\n    patterns = {\n        'grams': r'(\\d+(\\.\\d+)?)\\s*(g|grams?)',\n        'milligrams': r'(\\d+(\\.\\d+)?)\\s*(mg|milligrams?)',\n        'kilograms': r'(\\d+(\\.\\d+)?)\\s*(kg|kilograms?)',\n        'ounces': r'(\\d+(\\.\\d+)?)\\s*(oz|ounces?)',\n        'newtons': r'(\\d+(\\.\\d+)?)\\s*(n|newtons?)',\n        'liters': r'(\\d+(\\.\\d+)?)\\s*(l|liters?|ml|milliliters?)',\n        'watts': r'(\\d+(\\.\\d+)?)\\s*(w|watts?)',\n        'volts': r'(\\d+(\\.\\d+)?)\\s*(v|volts?)',\n        'inches': r'(\\d+(\\.\\d+)?)\\s*(in|inches?)',\n        'centimeters': r'(\\d+(\\.\\d+)?)\\s*(cm|centimeters?)',\n        'meters': r'(\\d+(\\.\\d+)?)\\s*(m|meters?)',\n        'feet': r'(\\d+(\\.\\d+)?)\\s*(ft|feet?)',\n        'pounds': r'(\\d+(\\.\\d+)?)\\s*(lbs|pounds?)',\n    }\n\n    pattern = patterns.get(entity.lower())\n    if pattern:\n        match = re.search(pattern, text.lower())\n        if match:\n            return f\"{match.group(1)} {entity.lower()}\", match.group(1)\n    \n    return f\"{entity} not found\", None\n\n# Function to process a single row of the DataFrame\ndef process_row(row, image_directory):\n    image_url = row['image_link']\n    entity_name = row['entity_name']\n\n    # Download the image if it's a URL\n    if image_url.startswith('http'):\n        image_path = download_image(image_url, image_directory)\n    else:\n        image_path = os.path.join(image_directory, image_url)\n\n    if not image_path or not os.path.isfile(image_path):\n        return \"Download Error\", \"\", None\n\n    # Extract text without rotation\n    final_text = extract_text_from_image(image_path)\n\n    if not final_text:\n        return \"Image Error\", \"\", None\n\n    # Search for the entity in the extracted text\n    entity_result, entity_value = search_entity(final_text, entity_name)\n\n    return entity_result, final_text, entity_value\n\n# Directory to save downloaded images\nimage_directory = '/kaggle/working/downloaded_test_images/'\n\n# Create directory if it doesn't exist\nif not os.path.exists(image_directory):\n    os.makedirs(image_directory)\n\n# Load the CSV containing image URLs and entity names\ncsv_path = '/kaggle/input/amazon-dataset/student_resource 3/dataset/test.csv'\ndf = pd.read_csv(csv_path)\n\n# Process a limited number of rows from the DataFrame\nlimit = 1000  # Set limit on the number of images to process\ndf_limited = df.head(limit)\n\n# Process the rows in parallel using ThreadPoolExecutor\nwith concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n    results = list(executor.map(lambda row: process_row(row, image_directory), [row for _, row in df_limited.iterrows()]))\n\n# Unpack results\npredictions, extracted_texts, entity_values = zip(*results)\n\n# Add the predictions, extracted texts, and entity values to the DataFrame using .loc\ndf_limited.loc[:, 'extracted_text'] = extracted_texts\ndf_limited.loc[:, 'prediction'] = predictions\ndf_limited.loc[:, 'entity_value'] = entity_values\n\n# Save the updated DataFrame to a CSV file\noutput_csv_path = '/kaggle/working/test_entity_predictions_with_text_and_value.csv'\ndf_limited.to_csv(output_csv_path, index=False)\n\nprint(f\"Results saved to {output_csv_path}\")\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train = pd.read_csv('entity_predictions_with_text_and_value.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train = train[['group_id', 'image_link', 'entity_name', 'extracted_text']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test = pd.read_csv('test_entity_predictions_with_text_and_value.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test.head(100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"# Extract the 4th row of a specific column (e.g., 'Column_Name')\ncolumn_name = 'extracted_text'\nfourth_row_value = test.at[4, column_name]  # 3 is the index for the 4th row (0-based indexing)\n\nprint(f\"The 4th row value of '{column_name}' is: {fourth_row_value}\")\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test1 = test[['group_id', 'image_link', 'entity_name', 'extracted_text', 'entity_value']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train1 = train[['group_id', 'image_link', 'entity_name', 'extracted_text']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train1.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df1 = pd.read_csv('/kaggle/input/amazon-dataset/student_resource 3/dataset/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# merge","metadata":{}},{"cell_type":"code","source":"# Merging df1 and test1 on image_link, entity_name, and group_id\n#df1 = df1.merge(train1, on=['image_link', 'entity_name', 'group_id'], how='inner')\n\n# Optionally, if you need to remove duplicates based on these columns:\n#df1 = df1.drop_duplicates(subset=['image_link', 'entity_name', 'group_id'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the 5th value of the 'extracted_text' column\n#value = df1['extracted_text'].iloc[4]\n#print(\"The 5th value in the 'extracted_text' column is:\", value)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Probability","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom fuzzywuzzy import fuzz\n\n# Function to calculate the prediction and the matching score\ndef predict_presence(entity_value, predicted, threshold=70):\n    # Calculate the fuzzy match ratio between the entity and the extracted text\n    match_score = fuzz.partial_ratio(entity_value.lower(), extracted_text.lower())\n    \n    # Predict presence based on the match score exceeding the threshold\n    presence = 1 if match_score >= threshold else 0\n    \n    return presence, match_score / 100  # Return as a probability (0 to 1)\n\n# Apply the function to each row in the DataFrame\npred[['prediction', 'probability']] = pred.apply(\n    lambda row: predict_presence(row['entity_value'], row['extracted_text']), axis=1, result_type='expand'\n)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the cleaned DataFrame\npred.head(30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom fuzzywuzzy import fuzz\n\n# Function to calculate the prediction and the matching score\ndef predict_presence(entity_value, extracted_text, threshold=70):\n    # Calculate the fuzzy match ratio between the entity and the extracted text\n    match_score = fuzz.partial_ratio(entity_value.lower(), extracted_text.lower())\n    \n    # Predict presence based on the match score exceeding the threshold\n    presence = 1 if match_score >= threshold else 0\n    \n    return presence, match_score / 100  # Return as a probability (0 to 1)\n\n# Apply the function to each row in the DataFrame\ndf1[['prediction', 'probability']] = df1.apply(\n    lambda row: predict_presence(row['entity_value'], row['extracted_text']), axis=1, result_type='expand'\n)\n\n# Show the DataFrame with predictions and probabilities\nprint(df1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nimport pandas as pd\nimport re\n\n# Define the patterns for different units (case insensitive)\npatterns = {\n    'grams': r'(\\d+(\\.\\d+)?)\\s*(g|grams?)',\n    'milligrams': r'(\\d+(\\.\\d+)?)\\s*(mg|milligrams?)',\n    'kilograms': r'(\\d+(\\.\\d+)?)\\s*(kg|kilograms?)',\n    'ounces': r'(\\d+(\\.\\d+)?)\\s*(oz|ounces?)',\n    'newtons': r'(\\d+(\\.\\d+)?)\\s*(n|newtons?)',\n    'liters': r'(\\d+(\\.\\d+)?)\\s*(l|liters?|ml|milliliters?)',\n    'watts': r'(\\d+(\\.\\d+)?)\\s*(w|watts?)',\n    'volts': r'(\\d+(\\.\\d+)?)\\s*(v|volts?)',\n    'inches': r'(\\d+(\\.\\d+)?)\\s*(in|inches?)',\n    'centimeters': r'(\\d+(\\.\\d+)?)\\s*(cm|centimeters?)',\n    'meters': r'(\\d+(\\.\\d+)?)\\s*(m|meters?)',\n    'feet': r'(\\d+(\\.\\d+)?)\\s*(ft|feet?)',\n    'pounds': r'(\\d+(\\.\\d+)?)\\s*(lbs|pounds?)',\n}\n\n# Map entity names to their corresponding unit patterns\nentity_unit_map = {\n    'weight': ['grams', 'milligrams', 'kilograms', 'ounces', 'pounds'],\n    'volume': ['liters', 'milliliters'],\n    'force': ['newtons'],\n    'electric_power': ['watts'],\n    'voltage': ['volts'],\n    'length': ['inches', 'centimeters', 'meters', 'feet']\n}\n\n# Function to clean extracted text and remove noise\ndef clean_extracted_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    # Remove special characters and keep alphanumeric and spaces\n    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n\n# Function to find the value with the appropriate unit in extracted_text based on the entity name\ndef find_unit_in_text(entity_name, extracted_text):\n    # Ensure extracted_text is a string\n    if pd.isna(extracted_text):\n        return None  # If no extracted text, return None\n    \n    # Clean the extracted text by removing noise\n    extracted_text = clean_extracted_text(str(extracted_text))\n\n    # Determine the type of entity (weight, volume, etc.)\n    entity_type = None\n    for key in entity_unit_map:\n        if key in entity_name.lower():\n            entity_type = key\n            break\n    \n    if entity_type is None:\n        return None  # If entity type is not recognized, return None\n    \n    # Search for numeric value followed by a unit based on entity type\n    for unit_name in entity_unit_map[entity_type]:\n        pattern = patterns[unit_name]  # Get the regex pattern for this unit\n        match = re.search(pattern, extracted_text, re.IGNORECASE)\n        if match:\n            return match.group(0)  # Return the matched value with the unit\n    \n    # If no match is found, return None\n    return None\n\n# Apply the function to each row in the DataFrame\ndf1['value_with_unit'] = df1.apply(\n    lambda row: find_unit_in_text(row['entity_value'], row['extracted_text']), axis=1\n)\n\n# Show the DataFrame with the extracted value with units\ndf1 = df1[['image_link', 'entity_value', 'extracted_text', 'value_with_unit']]\n\n# Save the updated DataFrame to a new CSV file\ndf1.to_csv('updated_values_with_units.csv', index=False)\n\n# Show the updated DataFrame\nprint(df1)\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df1.head(100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# pytesseract","metadata":{}},{"cell_type":"code","source":"#pip install doctr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nimport easyocr\nfrom PIL import Image, ImageEnhance, ImageFilter, ImageOps\nimport numpy as np\nimport re\n\n# Function to rotate an image\ndef rotate_image(image_path, angle):\n    with Image.open(image_path) as img:\n        rotated_img = img.rotate(angle, expand=True)\n        return rotated_img\n\n# Function to preprocess the image\ndef preprocess_image(image):\n    # Convert to grayscale\n    image = image.convert('L')\n    # Apply adaptive thresholding\n    image = ImageOps.autocontrast(image)\n    # Enhance the image\n    enhancer = ImageEnhance.Contrast(image)\n    image = enhancer.enhance(2)  # Increase contrast\n    # Apply a filter\n    image = image.filter(ImageFilter.SHARPEN)\n    return image\n\n# Function to perform OCR and check extracted text\ndef perform_ocr(image):\n    reader = easyocr.Reader(['en'])\n    result = reader.readtext(np.array(image))\n    return result\n\n# Function to clean extracted text\ndef clean_text(text):\n    # Remove unwanted characters and normalize spaces\n    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove non-alphanumeric characters\n    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n    return text.strip()\n\n# Function to evaluate if the extracted text is \"good\"\ndef is_text_good(result):\n    # Define criteria for good text (e.g., non-empty, specific content)\n    return any(len(clean_text(detection[1])) > 5 for detection in result)\n\n# Main function to handle rotating and OCR\ndef process_image(image_path):\n    angles = [0, 90, 180, 270]  # Angles to rotate the image\n    for angle in angles:\n        rotated_image = rotate_image(image_path, angle)\n        preprocessed_image = preprocess_image(rotated_image)\n        result = perform_ocr(preprocessed_image)\n        \n        if is_text_good(result):\n            return result, angle  # Return the successful result and angle\n    \n    return None, None  # No satisfactory result found\n\n# Path to your image\nimage_path = '/kaggle/working/downloaded_images/71u9qpY7TtL.jpg'\n\n# Process the image\nresult, angle = process_image(image_path)\n\nif result:\n    print(f\"Text extracted from image rotated by {angle} degrees:\")\n    for detection in result:\n        cleaned_text = clean_text(detection[1])\n        print(cleaned_text)\nelse:\n    print(\"No satisfactory text found.\")\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df1.head(100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the 5th value of the 'extracted_text' column\n#value = df1['extracted_text'].iloc[97]\n#print(\"The 5th value in the 'extracted_text' column is:\", value)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom fuzzywuzzy import fuzz\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models, transforms\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\n\n# Function to remove units from text (keeping only numeric values)\ndef remove_units(text):\n    return re.sub(r'[^0-9.]', '', str(text))\n\n# Function to calculate the prediction and matching score based on text\ndef predict_presence(entity_value, extracted_text, threshold=40):\n    clean_entity_value = remove_units(entity_value)\n    clean_extracted_text = remove_units(extracted_text)\n    match_score = fuzz.partial_ratio(clean_entity_value, clean_extracted_text)\n    presence = 1 if match_score >= threshold else 0\n    return presence, match_score / 100  # Return as a probability (0 to 1)\n\n# Apply transformations to the images for ResNet\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Custom Dataset class for loading both image and text data\nclass CustomDataset(Dataset):\n    def __init__(self, pred, transform=None):\n        self.pred = pred\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.pred)\n    \n    def __getitem__(self, idx):\n        # Load image\n        img_path = self.pred.iloc[idx]['image_link']\n        image = Image.open(img_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        # Get text features\n        entity_value = self.pred.iloc[idx]['entity_value']\n        extracted_text = self.pred.iloc[idx]['predicted']\n        prediction, probability = predict_presence(entity_value, extracted_text)\n        \n        # Target label: prediction presence\n        target = torch.tensor(prediction, dtype=torch.float32)\n        \n        return image, torch.tensor([probability], dtype=torch.float32), target\n\n# Create the dataset and dataloader\ndataset = CustomDataset(df1, transform=transform)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Load pre-trained ResNet model\nresnet = models.resnet18(pretrained=True)\nresnet.fc = nn.Identity()  # Remove the last fully connected layer\n\n# Classifier to predict presence based on both image features and text-based probability\nclass Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        self.resnet = resnet\n        self.fc = nn.Linear(512 + 1, 1)  # 512 from ResNet features + 1 for text-based probability\n        \n    def forward(self, image, text_prob):\n        image_features = self.resnet(image)  # Extract image features using ResNet\n        combined_features = torch.cat((image_features, text_prob), dim=1)  # Concatenate with text probability\n        output = self.fc(combined_features)  # Final prediction\n        return output\n\n# Initialize the model, loss function, and optimizer\nmodel = Classifier()\ncriterion = nn.BCEWithLogitsLoss()  # Binary classification\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T15:52:02.351596Z","iopub.execute_input":"2024-09-15T15:52:02.352007Z","iopub.status.idle":"2024-09-15T15:52:02.908738Z","shell.execute_reply.started":"2024-09-15T15:52:02.351968Z","shell.execute_reply":"2024-09-15T15:52:02.907156Z"},"trusted":true},"execution_count":20,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m image, torch\u001b[38;5;241m.\u001b[39mtensor([probability], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32), target\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Create the dataset and dataloader\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m dataset \u001b[38;5;241m=\u001b[39m CustomDataset(\u001b[43mdf1\u001b[49m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m     59\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Load pre-trained ResNet model\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'df1' is not defined"],"ename":"NameError","evalue":"name 'df1' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom fuzzywuzzy import fuzz\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models, transforms\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport requests\nfrom io import BytesIO\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Function to remove units from text (keeping only numeric values)\ndef remove_units(text):\n    return re.sub(r'[^0-9.]', '', str(text))\n\n# Function to calculate the prediction and matching score based on text\ndef predict_presence(entity_value, extracted_text, threshold=40):\n    clean_entity_value = remove_units(entity_value)\n    clean_extracted_text = remove_units(extracted_text)\n    match_score = fuzz.partial_ratio(clean_entity_value, clean_extracted_text)\n    presence = 1 if match_score >= threshold else 0\n    return presence, match_score / 100  # Return as a probability (0 to 1)\n\n# Apply transformations to the images for ResNet\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Custom Dataset class for loading both image and text data\nclass CustomDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        # Load image from URL\n        img_url = self.df.iloc[idx]['image_link']\n        \n        try:\n            response = requests.get(img_url)\n            image = Image.open(BytesIO(response.content)).convert('RGB')\n        except Exception as e:\n            print(f\"Error loading image from {img_url}: {e}\")\n            # Return a dummy tensor if there's an error\n            image = Image.new('RGB', (224, 224), color=(255, 255, 255))\n\n        if self.transform:\n            image = self.transform(image)\n        \n        # Get text features\n        entity_value = self.df.iloc[idx]['entity_value']\n        extracted_text = self.df.iloc[idx]['extracted_text']\n        prediction, probability = predict_presence(entity_value, extracted_text)\n        \n        # Target label: prediction presence\n        target = torch.tensor(prediction, dtype=torch.float32)\n        \n        return image, torch.tensor([probability], dtype=torch.float32), target\n\n# Load your dataset (ensure df1 is defined)\ndf1 = pd.read_csv('/path/to/your/dataset.csv')  # Adjust path as necessary\ndataset = CustomDataset(df1, transform=transform)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Load pre-trained ResNet model\nresnet = models.resnet18(pretrained=True)\nresnet.fc = nn.Identity()  # Remove the last fully connected layer\n\n# Classifier to predict presence based on both image features and text-based probability\nclass Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        self.resnet = resnet\n        self.fc = nn.Linear(512 + 1, 1)  # 512 from ResNet features + 1 for text-based probability\n        \n    def forward(self, image, text_prob):\n        image_features = self.resnet(image)  # Extract image features using ResNet\n        combined_features = torch.cat((image_features, text_prob), dim=1)  # Concatenate with text probability\n        output = self.fc(combined_features)  # Final prediction\n        return output\n\n# Initialize the model, loss function, and optimizer\nmodel = Classifier()\ncriterion = nn.BCEWithLogitsLoss()  # Binary classification\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Function to calculate predictions and apply sigmoid for probability thresholding\ndef calculate_predictions(outputs, threshold=0.5):\n    return (torch.sigmoid(outputs) >= threshold).float()\n\n# Training loop with accuracy and F1 score calculation\ndef train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n    model.train()\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        all_preds = []\n        all_targets = []\n        \n        for images, text_probs, targets in dataloader:\n            optimizer.zero_grad()\n            outputs = model(images, text_probs).squeeze()\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n            # Get predictions and store them\n            preds = calculate_predictions(outputs)\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n        \n        # Calculate accuracy and F1 score after each epoch\n        acc = accuracy_score(all_targets, all_preds)\n        f1 = f1_score(all_targets, all_preds)\n        \n        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader)}, Accuracy: {acc}, F1 Score: {f1}')\n\n# Train the model\ntrain_model(model, dataloader, criterion, optimizer, num_epochs=10)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom fuzzywuzzy import fuzz\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models, transforms\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport requests\nfrom io import BytesIO\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Function to remove non-alphanumeric characters except units\ndef clean_text(text):\n    return re.sub(r'[^\\w\\s.%]', '', str(text))  # Keep only words, spaces, and common units like % and .\n\n# Function to calculate the prediction based on text comparison, return matched value with unit if present\ndef predict_presence(entity_value, extracted_text, threshold=40):\n    clean_entity_value = clean_text(entity_value)\n    clean_extracted_text = clean_text(extracted_text)\n    \n    match_score = fuzz.partial_ratio(clean_entity_value, clean_extracted_text)\n    if match_score >= threshold:\n        # Return the part of the extracted text that matches the entity value along with its unit\n        return clean_entity_value, match_score / 100\n    else:\n        # No match found, return an empty string and match score 0\n        return \"\", 0\n\n# Apply transformations to the images for ResNet\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Custom Dataset class for loading both image and text data\nclass CustomDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        # Load image from URL\n        img_url = self.df.iloc[idx]['image_link']\n        \n        try:\n            response = requests.get(img_url)\n            image = Image.open(BytesIO(response.content)).convert('RGB')\n        except Exception as e:\n            print(f\"Error loading image from {img_url}: {e}\")\n            # Return a dummy tensor if there's an error\n            image = Image.new('RGB', (224, 224), color=(255, 255, 255))\n\n        if self.transform:\n            image = self.transform(image)\n        \n        # Get text features\n        entity_value = self.df.iloc[idx]['entity_value']\n        extracted_text = self.df.iloc[idx]['extracted_text']\n        matched_value, probability = predict_presence(entity_value, extracted_text)\n        \n        # If matched_value is empty, return 'no match'\n        if matched_value == \"\":\n            matched_value = \" \"\n        \n        return image, torch.tensor([probability], dtype=torch.float32), matched_value\n\n# Create the dataset and dataloader\ndataset = CustomDataset(df1, transform=transform)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Load pre-trained ResNet model\nresnet = models.resnet18(pretrained=True)\nresnet.fc = nn.Identity()  # Remove the last fully connected layer\n\n# Classifier to predict presence based on both image features and text-based probability\nclass Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        self.resnet = resnet\n        self.fc = nn.Linear(512 + 1, 1)  # 512 from ResNet features + 1 for text-based probability\n        \n    def forward(self, image, text_prob):\n        image_features = self.resnet(image)  # Extract image features using ResNet\n        combined_features = torch.cat((image_features, text_prob), dim=1)  # Concatenate with text probability\n        output = self.fc(combined_features)  # Final prediction\n        return output\n\n# Initialize the model, loss function, and optimizer\nmodel = Classifier()\ncriterion = nn.BCEWithLogitsLoss()  # Binary classification\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Function to calculate predictions and apply sigmoid for probability thresholding\ndef calculate_predictions(outputs, threshold=0.5):\n    return (torch.sigmoid(outputs) >= threshold).float()\n\n# Training loop with accuracy and F1 score calculation\ndef train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n    model.train()\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        all_preds = []\n        all_targets = []\n        all_matches = []\n        \n        for images, text_probs, matched_values in dataloader:\n            optimizer.zero_grad()\n            outputs = model(images, text_probs).squeeze()\n            # Dummy targets, as we're more interested in storing matched values\n            targets = torch.zeros(len(outputs))  # Since we're using BCEWithLogitsLoss, the target must be provided\n            \n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n            # Store matched values\n            all_matches.extend(matched_values)\n\n        # Print or log the results after each epoch\n        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader)}')\n        \n        # Display the matched values\n        print(f'Matched values: {all_matches[:5]}...')  # Show the first 5 matched values for brevity\n\n# Train the model\ntrain_model(model, dataloader, criterion, optimizer, num_epochs=10)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merging df1 and test1 on image_link, entity_name, and group_id\ndf1 = df1.merge(test1, on=['image_link', 'extracted_text'], how='inner')\n\n# Optionally, if you need to remove duplicates based on these columns:\n#df1 = df1.drop_duplicates(subset=['image_link', 'entity_name', 'group_id'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom fuzzywuzzy import fuzz\nimport re\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nfrom torchvision import transforms\n\n# Function to clean text\ndef clean_text(text):\n    return re.sub(r'[^\\w\\s.%]', '', str(text))  # Keep only alphanumeric characters, spaces, and common units\n\n# Function to predict the best entity value based on the entity name\ndef predict_entity_value(entity_name, extracted_text, threshold=40):\n    clean_extracted_text = clean_text(extracted_text)\n    \n    # Compare extracted text with entity_name and extract the most probable entity value\n    match_score = fuzz.partial_ratio(clean_text(entity_name), clean_extracted_text)\n    \n    if match_score >= threshold:\n        return clean_extracted_text, match_score / 100  # Return extracted text as predicted value and probability\n    else:\n        return \"\", 0  # No match found\n\n# Custom dataset class for test data\nclass TestDataset(torch.utils.data.Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        # Load image from URL\n        img_url = self.df.iloc[idx]['image_link']\n        \n        try:\n            response = requests.get(img_url)\n            image = Image.open(BytesIO(response.content)).convert('RGB')\n        except Exception as e:\n            print(f\"Error loading image from {img_url}: {e}\")\n            image = Image.new('RGB', (224, 224), color=(255, 255, 255))  # Dummy image if error occurs\n\n        if self.transform:\n            image = self.transform(image)\n        \n        # Extract entity name and extracted text\n        entity_name = self.df.iloc[idx]['entity_name']\n        extracted_text = self.df.iloc[idx]['extracted_text']\n        \n        # Predict the entity value based on extracted text\n        matched_value, probability = predict_entity_value(entity_name, extracted_text)\n        \n        if matched_value == \"\":\n            matched_value = \"no match\"\n        \n        return image, torch.tensor([probability], dtype=torch.float32), matched_value\n\n# Create the test dataloader\ndef get_test_dataloader(test_df, transform, batch_size=32):\n    test_dataset = TestDataset(test_df, transform=transform)\n    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    return test_dataloader\n\n# Testing function to make predictions and format them\ndef test_model(model, test_dataloader):\n    model.eval()\n    unit_predictions = []\n\n    with torch.no_grad():\n        for images, text_probs, matched_values in test_dataloader:\n            outputs = model(images, text_probs).squeeze()\n\n            # Collect and format predictions\n            for i in range(len(matched_values)):\n                matched_value = matched_values[i].strip()\n                if matched_value != \"no match\":\n                    prediction = f\"{float(text_probs[i].item()):.2f} {matched_value}\"\n                    unit_predictions.append(prediction)\n                else:\n                    unit_predictions.append(\"no match\")\n\n    return unit_predictions\n\n# Testing the model on a new CSV file\ndef test_on_csv(model, test_csv_path, output_csv_path):\n    # Load the test CSV\n    test_df = df1\n\n    # Create the test dataloader\n    test_dataloader = get_test_dataloader(test_df, transform)\n\n    # Get predictions in the required format\n    unit_predictions = test_model(model, test_dataloader)\n\n    # Assign predictions to the DataFrame\n    test_df['predicted_value'] = unit_predictions\n\n    # Save the predictions to a new CSV file\n    test_df.to_csv(output_csv_path, index=False)\n\n    print(\"Test predictions saved to:\", output_csv_path)\n    print(test_df[['entity_name', 'extracted_text', 'predicted_value']].head())\n\n# Assuming the test CSV is located at the following path\ntest_csv_path = '/kaggle/input/amazon-dataset/student_resource 3/dataset/test.csv'\noutput_csv_path = 'test_predictions_with_entities.csv'\n\n# Run the testing on the CSV\ntest_on_csv(model, test_csv_path, output_csv_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TR = pd.read_csv('/kaggle/working/test_predictions_with_entities.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TR.head(100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# For test dataset","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom fuzzywuzzy import fuzz\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models, transforms\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport requests\nfrom io import BytesIO\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Function to remove units from text (keeping only numeric values)\ndef remove_units(text):\n    return re.sub(r'[^0-9.]', '', str(text))\n\n# Function to calculate the prediction and matching score based on text\ndef predict_presence(entity_value, extracted_text, threshold=50):\n    clean_entity_value = remove_units(entity_value)\n    clean_extracted_text = remove_units(extracted_text)\n    match_score = fuzz.partial_ratio(clean_entity_value, clean_extracted_text)\n    presence = 1 if match_score >= threshold else 0\n    return presence, match_score / 100  # Return as a probability (0 to 1)\n\n# Apply transformations to the images for ResNet\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Custom Dataset class for loading both image and text data\nclass CustomDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        # Load image from URL\n        img_url = self.df.iloc[idx]['image_link']\n        \n        try:\n            response = requests.get(img_url)\n            image = Image.open(BytesIO(response.content)).convert('RGB')\n        except Exception as e:\n            print(f\"Error loading image from {img_url}: {e}\")\n            # Return a dummy tensor if there's an error\n            image = Image.new('RGB', (224, 224), color=(255, 255, 255))\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        # Get text features\n        entity_value = self.df.iloc[idx].get('entity_value', '0')\n        extracted_text = self.df.iloc[idx].get('extracted_text', '')\n        prediction, probability = predict_presence(entity_value, extracted_text)\n        \n        # Ensure targets are converted to floats\n        target = torch.tensor(float(prediction), dtype=torch.float32)\n        text_prob = torch.tensor(float(probability), dtype=torch.float32).unsqueeze(0)  # Ensure it's a 2D tensor\n        \n        return image, text_prob, target, img_url, extracted_text\n\n# Load pre-trained ResNet model\nresnet = models.resnet18(weights='DEFAULT')  # Use the new weights argument\nresnet.fc = nn.Identity()  # Remove the last fully connected layer\n\n# Classifier to predict presence based on both image features and text-based probability\nclass Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__() \n        self.resnet = resnet\n        self.fc = nn.Linear(512 + 1, 1)  # 512 from ResNet features + 1 for text-based probability\n        \n    def forward(self, image, text_prob):\n        image_features = self.resnet(image)  # Extract image features using ResNet\n        combined_features = torch.cat((image_features, text_prob), dim=1)  # Concatenate with text probability\n        output = self.fc(combined_features)  # Final prediction\n        return output\n\n# Function to calculate predictions and apply sigmoid for probability thresholding\ndef calculate_predictions(outputs, threshold=0.5):\n    return (torch.sigmoid(outputs) >= threshold).float()\n\n# Function to train the model\ndef train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n    model.train()\n    max_accuracy = 0\n    max_f1 = 0\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        \n        # Store predictions and targets for calculating metrics later\n        all_preds = []\n        all_targets = []\n        \n        for images, text_probs, targets, _, _ in dataloader:\n            optimizer.zero_grad()\n            outputs = model(images, text_probs).squeeze()\n            \n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            \n            # Calculate predictions and store\n            preds = calculate_predictions(outputs)\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n        \n        # Calculate accuracy and F1 score for this epoch\n        epoch_accuracy = accuracy_score(all_targets, all_preds)\n        epoch_f1 = f1_score(all_targets, all_preds)\n        \n        # Update max values if current epoch scores are better\n        if epoch_accuracy > max_accuracy:\n            max_accuracy = epoch_accuracy\n        if epoch_f1 > max_f1:\n            max_f1 = epoch_f1\n        \n        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader)}, Accuracy: {epoch_accuracy}, F1 Score: {epoch_f1}')\n    \n    print(f'Max Accuracy: {max_accuracy}, Max F1 Score: {max_f1}')\n\n# Function to test the model on the test data and return predictions\ndef test_model_with_predictions(model, dataloader):\n    model.eval()\n    all_preds = []\n    all_targets = []\n    all_image_links = []\n    all_extracted_texts = []\n    \n    with torch.no_grad():\n        for images, text_probs, targets, img_urls, extracted_texts in dataloader:\n            outputs = model(images, text_probs).squeeze()\n            preds = calculate_predictions(outputs)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n            all_image_links.extend(img_urls)\n            all_extracted_texts.extend(extracted_texts)\n    \n    # Convert predictions to a DataFrame for easy viewing\n    results_df = pd.DataFrame({\n        'image_link': all_image_links,\n        'extracted_text': all_extracted_texts,\n        'true_target': all_targets,\n        'predicted_presence': all_preds\n    })\n    \n    # Calculate accuracy and F1 score on test set\n    acc = accuracy_score(all_targets, all_preds)\n    f1 = f1_score(all_targets, all_preds)\n    \n    print(f'Test Accuracy: {acc}, Test F1 Score: {f1}')\n    return results_df, acc, f1\n\n# Load training data (assuming df1 is your training dataframe)\ndataset_train = CustomDataset(df1, transform=transform)\ndataloader_train = DataLoader(dataset_train, batch_size=32, shuffle=True)\n\n# Load test data (assuming df_test is your test dataframe)\ndf_test = pd.read_csv('/kaggle/input/amazon-dataset/student_resource 3/dataset/test.csv')  # Load your test dataset here\ndataset_test = CustomDataset(df_test, transform=transform)\ndataloader_test = DataLoader(dataset_test, batch_size=32, shuffle=False)\n\n# Initialize the model, loss function, and optimizer\nmodel = Classifier()\ncriterion = nn.BCEWithLogitsLoss()  # Binary classification\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Train the model\ntrain_model(model, dataloader_train, criterion, optimizer, num_epochs=10)\n\n# Test the model on the test dataset and get predictions\ntest_results_df, test_accuracy, test_f1 = test_model_with_predictions(model, dataloader_test)\n\n# Display the predictions (you can also save this to a CSV file)\nprint(test_results_df.head())\n\n# To save to CSV\ntest_results_df.to_csv('test_predictions.csv', index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Check the column names\ndf1.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TESTING ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom fuzzywuzzy import fuzz\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models, transforms\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport requests\nfrom io import BytesIO\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Function to remove units from text (keeping only numeric values)\ndef remove_units(text):\n    return re.sub(r'[^0-9.]', '', str(text))\n\n# Function to calculate the prediction and matching score based on text\ndef predict_presence(entity_value, extracted_text, threshold=40):\n    clean_entity_value = remove_units(entity_value)\n    clean_extracted_text = remove_units(extracted_text)\n    match_score = fuzz.partial_ratio(clean_entity_value, clean_extracted_text)\n    presence = 1 if match_score >= threshold else 0\n    return presence, match_score / 100  # Return as a probability (0 to 1)\n\n# Apply transformations to the images for ResNet\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Custom Dataset class for loading both image and text data\nclass CustomDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        # Load image from URL\n        img_url = self.df.iloc[idx]['image_link']\n        \n        try:\n            response = requests.get(img_url)\n            image = Image.open(BytesIO(response.content)).convert('RGB')\n        except Exception as e:\n            print(f\"Error loading image from {img_url}: {e}\")\n            # Return a dummy tensor if there's an error\n            image = Image.new('RGB', (224, 224), color=(255, 255, 255))\n\n        if self.transform:\n            image = self.transform(image)\n        \n        # Get text features\n        entity_value = self.df.iloc[idx]['entity_value']\n        extracted_text = self.df.iloc[idx]['extracted_text']\n        prediction, probability = predict_presence(entity_value, extracted_text)\n        \n        # Target label: prediction presence\n        target = torch.tensor(prediction, dtype=torch.float32)\n        \n        return image, torch.tensor([probability], dtype=torch.float32), target\n\n# Load pre-trained ResNet model\nresnet = models.resnet18(pretrained=True)\nresnet.fc = nn.Identity()  # Remove the last fully connected layer\n\n# Classifier to predict presence based on both image features and text-based probability\nclass Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        self.resnet = resnet\n        self.fc = nn.Linear(512 + 1, 1)  # 512 from ResNet features + 1 for text-based probability\n        \n    def forward(self, image, text_prob):\n        image_features = self.resnet(image)  # Extract image features using ResNet\n        combined_features = torch.cat((image_features, text_prob), dim=1)  # Concatenate with text probability\n        output = self.fc(combined_features)  # Final prediction\n        return output\n\n# Initialize the model, loss function, and optimizer\nmodel = Classifier()\ncriterion = nn.BCEWithLogitsLoss()  # Binary classification\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Function to calculate predictions and apply sigmoid for probability thresholding\ndef calculate_predictions(outputs, threshold=0.5):\n    return (torch.sigmoid(outputs) >= threshold).float()\n\n# Training loop with accuracy and F1 score calculation\ndef train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n    model.train()\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        all_preds = []\n        all_targets = []\n        \n        for images, text_probs, targets in dataloader:\n            optimizer.zero_grad()\n            outputs = model(images, text_probs).squeeze()\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n            # Get predictions and store them\n            preds = calculate_predictions(outputs)\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n        \n        # Calculate accuracy and F1 score after each epoch\n        acc = accuracy_score(all_targets, all_preds)\n        f1 = f1_score(all_targets, all_preds)\n        \n        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader)}, Accuracy: {acc}, F1 Score: {f1}')\n\n# Train the model\ntrain_model(model, dataloader, criterion, optimizer, num_epochs=10)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create testing dataset and dataloader\ndef create_test_dataloader(test_df, batch_size=32):\n    test_dataset = CustomDataset(test_df, transform=transform)\n    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    return test_dataloader\n\n# Predict on test data\ndef predict_on_test_data(model, test_dataloader):\n    model.eval()\n    all_predictions = []\n    with torch.no_grad():\n        for images, text_probs, _ in test_dataloader:\n            outputs = model(images, text_probs).squeeze()\n            preds = calculate_predictions(outputs)\n            all_predictions.extend(preds.cpu().numpy())\n    return all_predictions\n\n# Example test DataFrame\ntest_df = pd.read_csv('/kaggle/input/amazon-dataset/student_resource 3/dataset/test.csv')  # Assuming 'pred' contains the testing data as well\n\n# Create the test dataloader\ntest_dataloader = create_test_dataloader(test_df)\n\n# Predict on test data\npredictions = predict_on_test_data(model, test_dataloader)\n\n# Adding predictions to the test DataFrame\ntest_df['predictions'] = predictions\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[['entity_name', 'predictions']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}